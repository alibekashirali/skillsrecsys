{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ti-GoaISJqDi",
    "outputId": "2765478d-2096-4481-b3f8-e36eb7d80dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePt-8rNVhK3N"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fah8KpT8bTSx"
   },
   "outputs": [],
   "source": [
    "#\n",
    "TRAIN_DATA = [\n",
    "    (\"Хорошие знания PHP, Laravel и Node.js\", {\"entities\": [(15, 18, \"IT-SKILL\"), (20, 27, \"IT-SKILL\"), (30, 37, \"IT-SKILL\")]}),\n",
    "    (\"Знания SQL (PostgreSQL), CI/CD, Docker, kubernetes;\", {\"entities\": [(7, 10, \"IT-SKILL\"), (12, 22, \"IT-SKILL\"), (25, 30, \"IT-SKILL\"), (32, 38, \"IT-SKILL\"), (40, 50, \"IT-SKILL\")]}),\n",
    "    (\"Понимание restapi, OpenAPI (например swagger), kafka;\", {\"entities\": [(10, 17, \"IT-SKILL\"),(19, 26, \"IT-SKILL\"),(37, 44, \"IT-SKILL\"),(47, 52, \"IT-SKILL\")]}),\n",
    "    (\"Опыт администрирования Linux;\", {\"entities\": [(23, 28, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с Git\", {\"entities\": [(14, 17, \"IT-SKILL\")]}),\n",
    "    (\"Плюсом будет знание Go и/или С#\", {\"entities\": [(20, 22, \"IT-SKILL\"),(29, 31, \"IT-SKILL\")]}),\n",
    "    (\"Умение писать понятный код с комментированием и применением ключевых стандартов, рефакторинг при необходимости;\", {\"entities\": [(81, 92, \"IT-SKILL\")]}),\n",
    "    (\"Ответственность, самоорганизованность, коммуникабельность.\", {\"entities\": [(17, 37, \"IT-SKILL\"),(39, 57, \"IT-SKILL\")]}),\n",
    "    (\"Знакомство с различными видами тестирования;\", {\"entities\": [(31, 43, \"IT-SKILL\")]}),\n",
    "    (\"Трекинговые системы Jira, ClickUp, etc;\", {\"entities\": [(20, 24, \"IT-SKILL\"),(26, 33, \"IT-SKILL\")]}),\n",
    "    (\"Базовые навыки работы с DevTool, Postman;\", {\"entities\": [(24, 31, \"IT-SKILL\"),(33, 40, \"IT-SKILL\")]}),\n",
    "    (\"Понимание HTML, CSS;\", {\"entities\": [(10, 14, \"IT-SKILL\"),(16, 19, \"IT-SKILL\")]}),\n",
    "    (\"Знание техник тест-дизайна;\", {\"entities\": [(14, 25, \"IT-SKILL\")]}),\n",
    "    (\"Уверенное знание Postman, Insomnia;\", {\"entities\": [(17, 24, \"IT-SKILL\"), (26, 34, \"IT-SKILL\")]}),\n",
    "    (\"Базовое знание SQL;\", {\"entities\": [(15, 18, \"IT-SKILL\")]}),\n",
    "    (\"Понимание SOAP, REST;\", {\"entities\": [(10, 14, \"IT-SKILL\"), (16, 20, \"IT-SKILL\")]}),\n",
    "    (\"Понимание клиент-серверной архитектуры.\", {\"entities\": [(10, 38, \"IT-SKILL\")]}),\n",
    "    (\"I'm proficient in HTML, CSS, and JavaScript for web development.\", {\"entities\": [(18, 22, \"IT-SKILL\"), (24, 27, \"IT-SKILL\"), (33, 43, \"IT-SKILL\"), (48, 63, \"IT-SKILL\")]}),\n",
    "    (\"Developed applications using React, Angular, and Vue.js.\", {\"entities\": [(29, 34, \"IT-SKILL\"), (36, 43, \"IT-SKILL\"), (49, 55, \"IT-SKILL\")]}),\n",
    "    (\"Built RESTful APIs with Node.js and Express.js.\", {\"entities\": [(6, 18, \"IT-SKILL\"), (24, 31, \"IT-SKILL\"), (36, 46, \"IT-SKILL\")]}),\n",
    "    (\"Extensive experience with Ruby on Rails and Django.\", {\"entities\": [(26, 39, \"IT-SKILL\"), (44, 50, \"IT-SKILL\")]}),\n",
    "    (\"Implemented machine learning models using TensorFlow and Keras.\", {\"entities\": [(12, 28, \"IT-SKILL\"), (42, 52, \"IT-SKILL\"), (57, 62, \"IT-SKILL\")]}),\n",
    "    (\"Used R for data analysis and Python for data visualization.\", {\"entities\": [(5, 6, \"IT-SKILL\"), (11, 24, \"IT-SKILL\"), (29, 35, \"IT-SKILL\"), (40, 58, \"IT-SKILL\")]}),\n",
    "    (\"Worked with database management systems like MySQL and PostgreSQL.\", {\"entities\": [(45, 50, \"IT-SKILL\"), (55, 65, \"IT-SKILL\")]}),\n",
    "    (\"Experienced in cloud technologies such as AWS, Azure, and Google Cloud Platform.\", {\"entities\": [(15, 20, \"IT-SKILL\"), (42, 45, \"IT-SKILL\"), (47, 52, \"IT-SKILL\"), (58, 79, \"IT-SKILL\")]}),\n",
    "    (\"Implemented cybersecurity measures, including penetration testing and security audits.\", {\"entities\": [(12, 34, \"IT-SKILL\"), (46, 65, \"IT-SKILL\"), (70, 84, \"IT-SKILL\")]}),\n",
    "    (\"Managed Linux servers and wrote shell scripts for automation.\", {\"entities\": [(8, 13, \"IT-SKILL\"), (14, 21, \"IT-SKILL\"), (32, 37, \"IT-SKILL\"), (38, 45, \"IT-SKILL\")]}),\n",
    "    (\"Applied Agile methodologies like Scrum and Kanban for project management.\", {\"entities\": [(8, 13, \"IT-SKILL\"), (33, 38, \"IT-SKILL\"), (43, 49, \"IT-SKILL\")]}),\n",
    "    \n",
    "    (\"У меня есть опыт работы с Java, Kotlin и Swift для мобильной разработки.\", {\"entities\": [(26, 30, \"IT-SKILL\"), (32, 38, \"IT-SKILL\"), (41, 46, \"IT-SKILL\")]}),\n",
    "    (\"Я проектировал и разрабатывал системы на Python, Django и Flask.\", {\"entities\": [(41, 47, \"IT-SKILL\"), (49, 55, \"IT-SKILL\"), (58, 63, \"IT-SKILL\")]}),\n",
    "    (\"Имею опыт работы с различными базами данных, такими как MySQL, PostgreSQL и MongoDB.\", {\"entities\": [(56, 61, \"IT-SKILL\"), (63, 73, \"IT-SKILL\"), (76, 83, \"IT-SKILL\")]}),\n",
    "    (\"Занимался разработкой с использованием Git, Docker и Kubernetes.\", {\"entities\": [(39, 42, \"IT-SKILL\"), (44, 50, \"IT-SKILL\"), (53, 63, \"IT-SKILL\")]}),\n",
    "    (\"Специализировался на обработке данных с помощью TensorFlow, Keras и PyTorch.\", {\"entities\": [(48, 58, \"IT-SKILL\"), (60, 65, \"IT-SKILL\"), (68, 75, \"IT-SKILL\")]}),\n",
    "    (\"Я разрабатывал веб-сайты с использованием HTML, CSS и JavaScript.\", {\"entities\": [(42, 46, \"IT-SKILL\"), (48, 51, \"IT-SKILL\"), (54, 64, \"IT-SKILL\")]}),\n",
    "    (\"Использовал React, Angular и Vue.js для создания пользовательских интерфейсов.\", {\"entities\": [(12, 17, \"IT-SKILL\"), (19, 26, \"IT-SKILL\"), (29, 35, \"IT-SKILL\")]}),\n",
    "    (\"Работал с серверными технологиями, такими как Node.js, Express.js и Ruby on Rails.\", {\"entities\": [(46, 53, \"IT-SKILL\"), (55, 65, \"IT-SKILL\"), (68, 82, \"IT-SKILL\")]}),\n",
    "    (\"Мне приходилось работать с различными облачными сервисами, включая AWS, Azure и Google Cloud.\", {\"entities\": [(67, 70, \"IT-SKILL\"), (72, 77, \"IT-SKILL\"), (80, 92, \"IT-SKILL\")]}),\n",
    "    (\"Я занимался анализом данных с использованием R, Pandas и Matplotlib.\", {\"entities\": [(45, 46, \"IT-SKILL\"), (48, 54, \"IT-SKILL\"), (57, 67, \"IT-SKILL\")]}),\n",
    "    \n",
    "    (\"Java Software Engineer\", {\"entities\": [(0, 4, \"IT-SKILL\")]}),\n",
    "    (\"Upgraded functionality of distributed caching system based on Oracle Coherence\", {\"entities\": [(62, 78, \"IT-SKILL\")]}),\n",
    "    (\"Developed a new API using a new framework (Tomcat/Spring/Maven/Git) to replace legacy the API (JBOSS/Struts/EJB/SVN) as part of company's migration.\", {\"entities\": [(16, 19, \"IT-SKILL\"), (43, 49, \"IT-SKILL\"), (50, 56, \"IT-SKILL\"), (57, 62, \"IT-SKILL\"), (63, 66, \"IT-SKILL\"), (95, 100, \"IT-SKILL\"), (101, 107, \"IT-SKILL\"), (108, 111, \"IT-SKILL\"), (112, 115, \"IT-SKILL\")]}),\n",
    "    (\"Implemented load and performance test cases via Gatling.io\", {\"entities\": [(21, 43, \"IT-SKILL\"), (48, 58, \"IT-SKILL\")]}),\n",
    "    \n",
    "    (\"Full Stack Web Developer with expertise in Python, Django, and React.js\", {\"entities\": [(0, 10, \"IT-SKILL\"), (11, 24, \"IT-SKILL\"), (43, 49, \"IT-SKILL\"), (51, 57, \"IT-SKILL\"), (63, 71, \"IT-SKILL\")]}),\n",
    "    (\"Implemented machine learning algorithms using TensorFlow and Keras\", {\"entities\": [(12, 28, \"IT-SKILL\"), (46, 56, \"IT-SKILL\"), (61, 66, \"IT-SKILL\")]}),\n",
    "    (\"Configured and maintained cloud infrastructure on AWS, c, and Azure\", {\"entities\": [(26, 31, \"IT-SKILL\"), (50, 53, \"IT-SKILL\"), (62, 67, \"IT-SKILL\")]}),\n",
    "    (\"Developed web applications using Angular, Node.js, and MongoDB\", {\"entities\": [(10, 25, \"IT-SKILL\"), (33, 40, \"IT-SKILL\"), (42, 49, \"IT-SKILL\"), (55, 62, \"IT-SKILL\")]}),\n",
    "    (\"Created data visualizations using D3.js and Tableau\", {\"entities\": [(8, 27, \"IT-SKILL\"), (34, 39, \"IT-SKILL\"), (44, 51, \"IT-SKILL\")]}),\n",
    "    (\"Designed and implemented RESTful APIs using Flask and PostgreSQL\", {\"entities\": [(25, 36, \"IT-SKILL\"), (44, 49, \"IT-SKILL\"), (54, 64, \"IT-SKILL\")]}),\n",
    "    (\"Used agile methodologies, Scrum and Kanban, for project management\", {\"entities\": [(5, 10, \"IT-SKILL\"), (23, 28, \"IT-SKILL\"), (33, 39, \"IT-SKILL\")]}),\n",
    "    (\"Implemented CI/CD pipeline using Jenkins and Docker\", {\"entities\": [(12, 17, \"IT-SKILL\"), (33, 40, \"IT-SKILL\"), (45, 51, \"IT-SKILL\")]}),\n",
    "    (\"Optimized SQL queries and utilized stored procedures\", {\"entities\": [(10, 13, \"IT-SKILL\"), (35, 51, \"IT-SKILL\")]}),\n",
    "    (\"Developed mobile applications using Swift and Kotlin for iOS and Android\", {\"entities\": [(10, 28, \"IT-SKILL\"), (36, 41, \"IT-SKILL\"), (46, 52, \"IT-SKILL\"), (57, 60, \"IT-SKILL\"), (65, 72, \"IT-SKILL\")]}),\n",
    "    \n",
    "    (\"Веб-разработчик со знанием Python, Django и React.js\", {\"entities\": [(0, 15, \"IT-SKILL\"), (27, 33, \"IT-SKILL\"), (35, 41, \"IT-SKILL\"), (44, 52, \"IT-SKILL\")]}),\n",
    "    (\"Реализовал алгоритмы машинного обучения с использованием TensorFlow и Keras\", {\"entities\": [(57, 67, \"IT-SKILL\"), (70, 75, \"IT-SKILL\")]}),\n",
    "    (\"Настроил и поддерживал облачную инфраструктуру на AWS, GCP и Azure\", {\"entities\": [(23, 46, \"IT-SKILL\"), (50, 53, \"IT-SKILL\"), (55, 58, \"IT-SKILL\"), (61, 66, \"IT-SKILL\")]}),\n",
    "    (\"Разработал веб-приложения с использованием Angular, Node.js и MongoDB\", {\"entities\": [(11, 25, \"IT-SKILL\"), (43, 50, \"IT-SKILL\"), (52, 59, \"IT-SKILL\"), (62, 69, \"IT-SKILL\")]}),    \n",
    "    (\"Создал визуализации данных с использованием D3.js и Tableau\", {\"entities\": [(7, 26, \"IT-SKILL\"), (44, 49, \"IT-SKILL\"), (52, 59, \"IT-SKILL\")]}),\n",
    "    (\"Спроектировал и реализовал RESTful API с использованием Flask и PostgreSQL\", {\"entities\": [(27, 38, \"IT-SKILL\"), (56, 61, \"IT-SKILL\"), (64, 74, \"IT-SKILL\")]}),\n",
    "    (\"Использовал гибкие методологии, Scrum и Kanban, для управления проектами\", {\"entities\": [(32, 37, \"IT-SKILL\"), (40, 46, \"IT-SKILL\")]}),    \n",
    "    (\"Реализовал CI/CD пайплайн с использованием Jenkins и Docker\", {\"entities\": [(11, 16, \"IT-SKILL\"), (43, 50, \"IT-SKILL\"), (53, 59, \"IT-SKILL\")]}),\n",
    "    (\"Оптимизировал SQL запросы и использовал хранимые процедуры\", {\"entities\": [(14, 17, \"IT-SKILL\"), (40, 58, \"IT-SKILL\")]}),\n",
    "    (\"Разработал мобильные приложения с использованием Swift и Kotlin для iOS и Android\", {\"entities\": [(49, 54, \"IT-SKILL\"), (57, 63, \"IT-SKILL\"), (68, 71, \"IT-SKILL\"), (74, 81, \"IT-SKILL\")]}),\n",
    "\n",
    "    (\"Implemented continuous integration and delivery with Jenkins and GitLab.\", {\"entities\": [(12, 39, \"IT-SKILL\"), (44, 50, \"IT-SKILL\"), (55, 61, \"IT-SKILL\")]}),\n",
    "    (\"Experience with virtualization technologies such as Docker and Kubernetes.\", {\"entities\": [(14, 31, \"IT-SKILL\"), (36, 42, \"IT-SKILL\"), (47, 57, \"IT-SKILL\")]}),\n",
    "    (\"Familiar with front-end frameworks such as Bootstrap and Material Design.\", {\"entities\": [(16, 26, \"IT-SKILL\"), (36, 45, \"IT-SKILL\"), (50, 66, \"IT-SKILL\")]}),\n",
    "    (\"Knowledge of programming languages such as Java, Python, and C++.\", {\"entities\": [(22, 26, \"IT-SKILL\"), (28, 34, \"IT-SKILL\"), (39, 42, \"IT-SKILL\"), (47, 49, \"IT-SKILL\"), (54, 57, \"IT-SKILL\")]}),\n",
    "    (\"Developed and maintained web applications using PHP and MySQL.\", {\"entities\": [(35, 38, \"IT-SKILL\"), (44, 49, \"IT-SKILL\")]}),\n",
    "    (\"Experience with Agile software development and project management methodologies.\", {\"entities\": [(14, 19, \"IT-SKILL\"), (37, 44, \"IT-SKILL\"), (49, 68, \"IT-SKILL\")]}),\n",
    "    (\"Developed custom WordPress plugins using PHP and MySQL.\", {\"entities\": [(9, 24, \"IT-SKILL\"), (30, 33, \"IT-SKILL\"), (39, 44, \"IT-SKILL\")]}),\n",
    "    (\"Experience with network and server administration, including DNS and TCP/IP.\", {\"entities\": [(14, 21, \"IT-SKILL\"), (26, 32, \"IT-SKILL\"), (53, 62, \"IT-SKILL\"), (67, 72, \"IT-SKILL\")]}),\n",
    "    (\"Experience with data mining and statistical analysis using R and Python.\", {\"entities\": [(14, 24, \"IT-SKILL\"), (42, 43, \"IT-SKILL\"), (48, 55, \"IT-SKILL\"), (60, 66, \"IT-SKILL\")]}),\n",
    "    (\"Experience with mobile application development using React Native and Swift.\", {\"entities\": [(14, 38, \"IT-SKILL\"), (43, 48, \"IT-SKILL\")]}),\n",
    "    (\"Experience with cloud-native technologies such as Kubernetes, Docker, and AWS Lambda.\", {\"entities\": [(14, 32, \"IT-SKILL\"), (37, 43, \"IT-SKILL\"), (48, 58, \"IT-SKILL\"), (63, 73, \"IT-SKILL\")]}),\n",
    "    (\"Experience with software testing and test automation using Selenium and Appium.\", {\"entities\": [(14, 31, \"IT-SKILL\"), (36, 47, \"IT-SKILL\"), (52, 61, \"IT-SKILL\")]}),\n",
    "    (\"Experience with web analytics tools like Google Analytics and Adobe Analytics.\", {\"entities\": [(14, 27, \"IT-SKILL\"), (32, 47, \"IT-SKILL\"), (52, 67, \"IT-SKILL\")]}),\n",
    "\n",
    "    (\"Опыт разработки на Java;\", {\"entities\": [(15, 19, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с базами данных MySQL, PostgreSQL;\", {\"entities\": [(28, 33, \"IT-SKILL\"), (35, 46, \"IT-SKILL\")]}),\n",
    "    (\"Знание Python и библиотек для машинного обучения, таких как NumPy, Pandas, Scikit-learn, TensorFlow;\", {\"entities\": [(7, 13, \"IT-SKILL\"), (18, 25, \"IT-SKILL\"), (48, 53, \"IT-SKILL\"), (55, 61, \"IT-SKILL\"), (63, 76, \"IT-SKILL\"), (78, 89, \"IT-SKILL\"), (91, 102, \"IT-SKILL\")]}),\n",
    "    (\"Умение создавать веб-сайты с использованием JavaScript, HTML и CSS;\", {\"entities\": [(24, 34, \"IT-SKILL\"), (38, 45, \"IT-SKILL\"), (50, 53, \"IT-SKILL\"), (58, 61, \"IT-SKILL\"), (63, 66, \"IT-SKILL\"), (68, 71, \"IT-SKILL\"), (74, 77, \"IT-SKILL\"), (79, 82, \"IT-SKILL\"), (84, 87, \"IT-SKILL\"), (90, 93, \"IT-SKILL\"), (95, 98, \"IT-SKILL\"), (100, 103, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с системами контроля версий, такими как Mercurial;\", {\"entities\": [(13, 29, \"IT-SKILL\"), (52, 62, \"IT-SKILL\")]}),\n",
    "    (\"Знание алгоритмов и структур данных;\", {\"entities\": [(7, 17, \"IT-SKILL\"), (20, 33, \"IT-SKILL\")]}),\n",
    "    (\"Умение работать с Linux-серверами и настройкой сетей;\", {\"entities\": [(15, 28, \"IT-SKILL\"), (32, 40, \"IT-SKILL\")]}),\n",
    "    (\"Знание языка программирования С++;\", {\"entities\": [(7, 31, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с фреймворками для веб-разработки, такими как Django и Ruby on Rails;\", {\"entities\": [(13, 28, \"IT-SKILL\"), (49, 55, \"IT-SKILL\"), (61, 75, \"IT-SKILL\")]}),\n",
    "    (\"Умение работать с платформой Amazon Web Services (AWS);\", {\"entities\": [(15, 36, \"IT-SKILL\"), (38, 41, \"IT-SKILL\"), (43, 49, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с AWS, Azure или Google Cloud Platform\", {\"entities\": [(14, 17, \"IT-SKILL\"), (19, 24, \"IT-SKILL\"), (26, 31, \"IT-SKILL\")]}),\n",
    "    (\"Опыт разработки мобильных приложений на Android и/или iOS\", {\"entities\": [(32, 39, \"IT-SKILL\"), (44, 51, \"IT-SKILL\")]}),\n",
    "    (\"Знание принципов объектно-ориентированного программирования\", {\"entities\": [(7, 15, \"IT-SKILL\"), (28, 50, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с базами данных MongoDB, MySQL или PostgreSQL\", {\"entities\": [(25, 32, \"IT-SKILL\"), (34, 38, \"IT-SKILL\"), (43, 53, \"IT-SKILL\")]}),\n",
    "    (\"Знание принципов SOLID\", {\"entities\": [(7, 12, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с фреймворками Symfony или Laravel\", {\"entities\": [(25, 33, \"IT-SKILL\"), (38, 45, \"IT-SKILL\")]}),\n",
    "    (\"Знание JavaScript-фреймворков React, Angular или Vue.js\", {\"entities\": [(7, 34, \"IT-SKILL\"), (36, 43, \"IT-SKILL\"), (48, 54, \"IT-SKILL\")]}),\n",
    "    (\"Умение работать с системами контроля версий SVN или Mercurial\", {\"entities\": [(24, 40, \"IT-SKILL\"), (45, 56, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с микросервисами и контейнеризацией Docker или Kubernetes\", {\"entities\": [(7, 21, \"IT-SKILL\"), (37, 43, \"IT-SKILL\"), (48, 57, \"IT-SKILL\")]}),\n",
    "    (\"Знание языков программирования Python и/или Java\", {\"entities\": [(7, 21, \"IT-SKILL\"), (26, 32, \"IT-SKILL\"), (35, 39, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с библиотеками NumPy, Pandas и/или Matplotlib\", {\"entities\": [(25, 29, \"IT-SKILL\"), (31, 37, \"IT-SKILL\"), (42, 53, \"IT-SKILL\")]}),\n",
    "    (\"Знание принципов функционального программирования\", {\"entities\": [(7, 14, \"IT-SKILL\"), (28, 53, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с системами мониторинга Zabbix, Nagios или Prometheus\", {\"entities\": [(7, 21, \"IT-SKILL\"), (37, 43, \"IT-SKILL\"), (48, 58, \"IT-SKILL\")]}),\n",
    "\n",
    "    (\"Опыт разработки на Java EE и Spring Framework;\", {\"entities\": [(22, 29, \"IT-SKILL\"),(34, 50, \"IT-SKILL\")]}),\n",
    "    (\"Знание принципов ООП и паттернов проектирования;\", {\"entities\": [(0, 16, \"IT-SKILL\"),(19, 36, \"IT-SKILL\")]}),\n",
    "    (\"Уверенное владение Python и библиотеками для машинного обучения (например, Scikit-learn, Tensorflow, Keras);\", {\"entities\": [(17, 23, \"IT-SKILL\"),(43, 56, \"IT-SKILL\"),(58, 68, \"IT-SKILL\"),(70, 76, \"IT-SKILL\")]}),\n",
    "    (\"Опыт работы с базами данных MongoDB, Redis, Cassandra;\", {\"entities\": [(25, 32, \"IT-SKILL\"),(34, 38, \"IT-SKILL\"),(40, 48, \"IT-SKILL\")]}),\n",
    "    (\"Знание основных протоколов сетевого взаимодействия (HTTP, FTP, SMTP, DNS);\", {\"entities\": [(0, 6, \"IT-SKILL\"),(8, 11, \"IT-SKILL\"),(13, 17, \"IT-SKILL\"),(19, 23, \"IT-SKILL\"),(25, 28, \"IT-SKILL\"),(30, 33, \"IT-SKILL\")]}),\n",
    "    (\"Опыт разработки мобильных приложений под iOS и Android;\", {\"entities\": [(14, 31, \"IT-SKILL\"),(36, 43, \"IT-SKILL\"),(48, 55, \"IT-SKILL\")]}),\n",
    "    (\"Умение работать с системами контроля версий SVN и Mercurial;\", {\"entities\": [(31, 34, \"IT-SKILL\"),(39, 48, \"IT-SKILL\")]}),\n",
    "    (\"Знание принципов работы операционных систем Windows и MacOS;\", {\"entities\": [(0, 6, \"IT-SKILL\"),(9, 16, \"IT-SKILL\"),(21, 26, \"IT-SKILL\"),(29, 34, \"IT-SKILL\")]}),\n",
    "    (\"Опыт разработки на JavaScript и фреймворках React и Angular;\", {\"entities\": [(22, 33, \"IT-SKILL\"),(38, 43, \"IT-SKILL\"),(48, 55, \"IT-SKILL\")]}),\n",
    "    (\"Знание принципов работы и настройки веб-серверов Apache и Nginx;\", {\"entities\": [(0, 16, \"IT-SKILL\"),(19, 25, \"IT-SKILL\"),(30, 41, \"IT-SKILL\"),(44, 49, \"IT-SKILL\")]}),          \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_3BwiaXbkTd"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Разделите TRAIN_DATA на тренировочный и тестовый наборы (например, 80% на 20%)\n",
    "random.shuffle(TRAIN_DATA)\n",
    "train_size = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:train_size]\n",
    "test_data = TRAIN_DATA[train_size:]\n",
    "\n",
    "# Load the base model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a new NER pipeline if it doesn't exist\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "\n",
    "# Add the new label (IT-SKILL) to the NER pipeline\n",
    "ner.add_label(\"IT-SKILL\")\n",
    "\n",
    "# Train the NER model\n",
    "optimizer = nlp.resume_training()\n",
    "for i in range(30):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(1.0, 4.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in zip(texts, annotations)]\n",
    "        nlp.update(examples, sgd=optimizer, drop=0.5, losses=losses)\n",
    "    print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"it_skills_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtzsT88Qbwdy"
   },
   "outputs": [],
   "source": [
    "dropout_rate = [0.2, 0.35, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KbeW32Afn2g"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Разделите TRAIN_DATA на тренировочный и тестовый наборы (например, 80% на 20%)\n",
    "random.shuffle(TRAIN_DATA)\n",
    "train_size = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:train_size]\n",
    "test_data = TRAIN_DATA[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i44xmjvNfSam",
    "outputId": "d2991293-75d5-413e-8e52-e6c25d47ba35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт разработки мобильных приложений под ios и and...\" with entities \"[(14, 31, 'IT-SKILL'), (36, 43, 'IT-SKILL'), (48, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"developed web applications using angular, node.js,...\" with entities \"[(10, 25, 'IT-SKILL'), (33, 40, 'IT-SKILL'), (42, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"experience with software testing and test automati...\" with entities \"[(14, 31, 'IT-SKILL'), (36, 47, 'IT-SKILL'), (52, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"implemented cybersecurity measures, including pene...\" with entities \"[(12, 34, 'IT-SKILL'), (46, 65, 'IT-SKILL'), (70, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"умение работать с платформой amazon web services (...\" with entities \"[(15, 36, 'IT-SKILL'), (38, 41, 'IT-SKILL'), (43, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт разработки на java;\" with entities \"[(15, 19, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт разработки на javascript и фреймворках react ...\" with entities \"[(22, 33, 'IT-SKILL'), (38, 43, 'IT-SKILL'), (48, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с aws, azure или google cloud platform\" with entities \"[(14, 17, 'IT-SKILL'), (19, 24, 'IT-SKILL'), (26, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"developed custom wordpress plugins using php and m...\" with entities \"[(9, 24, 'IT-SKILL'), (30, 33, 'IT-SKILL'), (39, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"experience with mobile application development usi...\" with entities \"[(14, 38, 'IT-SKILL'), (43, 48, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с базами данных mongodb, mysql или pos...\" with entities \"[(25, 32, 'IT-SKILL'), (34, 38, 'IT-SKILL'), (43, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание принципов solid\" with entities \"[(7, 12, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание алгоритмов и структур данных;\" with entities \"[(7, 17, 'IT-SKILL'), (20, 33, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"experience with virtualization technologies such a...\" with entities \"[(14, 31, 'IT-SKILL'), (36, 42, 'IT-SKILL'), (47, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"уверенное владение python и библиотеками для машин...\" with entities \"[(17, 23, 'IT-SKILL'), (43, 56, 'IT-SKILL'), (58, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"experience with web analytics tools like google an...\" with entities \"[(14, 27, 'IT-SKILL'), (32, 47, 'IT-SKILL'), (52, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с фреймворками для веб-разработки, так...\" with entities \"[(13, 28, 'IT-SKILL'), (49, 55, 'IT-SKILL'), (61, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание принципов работы и настройки веб-серверов a...\" with entities \"[(0, 16, 'IT-SKILL'), (19, 25, 'IT-SKILL'), (30, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание языков программирования python и/или java\" with entities \"[(7, 21, 'IT-SKILL'), (26, 32, 'IT-SKILL'), (35, 3...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с библиотеками numpy, pandas и/или mat...\" with entities \"[(25, 29, 'IT-SKILL'), (31, 37, 'IT-SKILL'), (42, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"designed and implemented restful apis using flask ...\" with entities \"[(25, 36, 'IT-SKILL'), (44, 49, 'IT-SKILL'), (54, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание принципов объектно-ориентированного програм...\" with entities \"[(7, 15, 'IT-SKILL'), (28, 50, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт разработки на java ee и spring framework;\" with entities \"[(22, 29, 'IT-SKILL'), (34, 50, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с системами мониторинга zabbix, nagios...\" with entities \"[(7, 21, 'IT-SKILL'), (37, 43, 'IT-SKILL'), (48, 5...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание принципов ооп и паттернов проектирования;\" with entities \"[(0, 16, 'IT-SKILL'), (19, 36, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"implemented continuous integration and delivery wi...\" with entities \"[(12, 39, 'IT-SKILL'), (44, 50, 'IT-SKILL'), (55, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"knowledge of programming languages such as java, p...\" with entities \"[(22, 26, 'IT-SKILL'), (28, 34, 'IT-SKILL'), (39, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание техник тест-дизайна;\" with entities \"[(14, 25, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"familiar with front-end frameworks such as bootstr...\" with entities \"[(16, 26, 'IT-SKILL'), (36, 45, 'IT-SKILL'), (50, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"умение работать с linux-серверами и настройкой сет...\" with entities \"[(15, 28, 'IT-SKILL'), (32, 40, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"умение создавать веб-сайты с использованием javasc...\" with entities \"[(24, 34, 'IT-SKILL'), (38, 45, 'IT-SKILL'), (50, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"experience with network and server administration,...\" with entities \"[(14, 21, 'IT-SKILL'), (26, 32, 'IT-SKILL'), (53, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"developed and maintained web applications using ph...\" with entities \"[(35, 38, 'IT-SKILL'), (44, 49, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с базами данных mongodb, redis, cassan...\" with entities \"[(25, 32, 'IT-SKILL'), (34, 38, 'IT-SKILL'), (40, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"умение работать с системами контроля версий svn и ...\" with entities \"[(31, 34, 'IT-SKILL'), (39, 48, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание языка программирования с++;\" with entities \"[(7, 31, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"optimized sql queries and utilized stored procedur...\" with entities \"[(10, 13, 'IT-SKILL'), (35, 51, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"experience with agile software development and pro...\" with entities \"[(14, 19, 'IT-SKILL'), (37, 44, 'IT-SKILL'), (49, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с микросервисами и контейнеризацией do...\" with entities \"[(7, 21, 'IT-SKILL'), (37, 43, 'IT-SKILL'), (48, 5...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"знание принципов работы операционных систем window...\" with entities \"[(0, 6, 'IT-SKILL'), (9, 16, 'IT-SKILL'), (21, 26,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт разработки мобильных приложений на android и/...\" with entities \"[(32, 39, 'IT-SKILL'), (44, 51, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"developed mobile applications using swift and kotl...\" with entities \"[(10, 28, 'IT-SKILL'), (36, 41, 'IT-SKILL'), (46, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"опыт работы с системами контроля версий, такими ка...\" with entities \"[(13, 29, 'IT-SKILL'), (52, 62, 'IT-SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 284.045562115386}\n",
      "Epoch 2: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 255.18077896148174}\n",
      "Epoch 3: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 223.81685758232888}\n",
      "Epoch 4: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 212.1048501345007}\n",
      "Epoch 5: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 202.8601262115318}\n",
      "Epoch 6: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 169.83566284044724}\n",
      "Epoch 7: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 159.23852014265714}\n",
      "Epoch 8: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 138.5064405020765}\n",
      "Epoch 9: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 126.75815818665164}\n",
      "Epoch 10: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 122.78721222246696}\n",
      "Trained model saved to: model_lr_0.001\n",
      "Training with learning rate: 0.0001\n",
      "Epoch 1: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 116.74033399934069}\n",
      "Epoch 2: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 101.79541654149915}\n",
      "Epoch 3: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 95.11507210113993}\n",
      "Epoch 4: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 102.2699486689205}\n",
      "Epoch 5: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 99.71110909858572}\n",
      "Epoch 6: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.93470810083045}\n",
      "Epoch 7: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 98.57931958648938}\n",
      "Epoch 8: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 103.0700872574724}\n",
      "Epoch 9: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 98.19308884260255}\n",
      "Epoch 10: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 92.03677988777463}\n",
      "Trained model saved to: model_lr_0.0001\n",
      "Training with learning rate: 1e-05\n",
      "Epoch 1: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.3887503020657}\n",
      "Epoch 2: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 95.47619591401008}\n",
      "Epoch 3: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.06027450143203}\n",
      "Epoch 4: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 91.56240635730575}\n",
      "Epoch 5: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 102.93109540035493}\n",
      "Epoch 6: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 97.26724341993167}\n",
      "Epoch 7: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 100.44333532437486}\n",
      "Epoch 8: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 105.35824804756815}\n",
      "Epoch 9: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 92.15957461238014}\n",
      "Epoch 10: Losses - {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 95.51495387592067}\n",
      "Trained model saved to: model_lr_1e-05\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Загрузка предварительно обученной модели SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# # Подготовка данных для обучения\n",
    "# train_data = [...]  # Ваши данные для обучения\n",
    "\n",
    "\n",
    "# Add a new NER pipeline if it doesn't exist\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "\n",
    "# Add the new label (IT-SKILL) to the NER pipeline\n",
    "ner.add_label(\"IT-SKILL\")\n",
    "\n",
    "# Параметры обучения\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rates = [0.001, 0.0001, 0.00001]  # Разные значения learning rate\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    \n",
    "    # Инициализация модели и оптимизатора\n",
    "    optimizer = nlp.resume_training()\n",
    "    optimizer.learn_rate = lr\n",
    "    \n",
    "    # Обучение модели\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=batch_size)\n",
    "        \n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            examples = []\n",
    "            \n",
    "            for text, annot in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annot)\n",
    "                examples.append(example)\n",
    "                \n",
    "            nlp.update(examples, sgd=optimizer, drop=0.5, losses=losses)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Losses - {losses}\")\n",
    "    \n",
    "    # Сохранение обученной модели\n",
    "    output_dir = f\"model_lr_{lr}\"\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(f\"Trained model saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIoyOcl4f4jB",
    "outputId": "66672cd2-0af7-43a1-dcbf-507a32d62ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'tag_acc': None, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'pos_acc': None, 'morph_acc': None, 'morph_micro_p': None, 'morph_micro_r': None, 'morph_micro_f': None, 'morph_per_feat': None, 'lemma_acc': None, 'ents_p': 0.7894736842105263, 'ents_r': 0.6818181818181818, 'ents_f': 0.7317073170731707, 'ents_per_type': {'IT-SKILL': {'p': 0.7894736842105263, 'r': 0.6818181818181818, 'f': 0.7317073170731707}}, 'speed': 4538.183876698268}\n",
      "Precision: 0.7894736842105263\n",
      "Recall: 0.6818181818181818\n",
      "F-score: 0.7317073170731707\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load your trained model\n",
    "nlp = spacy.load('model_lr_1e-05')\n",
    "\n",
    "# Prepare the evaluation data\n",
    "\n",
    "evaluation_data = test_data\n",
    "\n",
    "# Note: the data format should match your training data\n",
    "examples = []\n",
    "for text, annots in evaluation_data:   # assuming evaluation_data is a list of tuples\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annots)\n",
    "    examples.append(example)\n",
    "\n",
    "# Evaluate the model\n",
    "scorer = nlp.evaluate(examples)\n",
    "\n",
    "print(scorer)\n",
    "# Output the evaluation results\n",
    "ents_p, ents_r, ents_f = scorer['ents_p'], scorer['ents_r'], scorer['ents_f']\n",
    "print(f'Precision: {ents_p}')\n",
    "print(f'Recall: {ents_r}')\n",
    "print(f'F-score: {ents_f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJkxYPGqlhI1"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# библиотеки для оценивания модели ner\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "\n",
    "def train_and_evaluate_ner_model(nlp, train_data, test_data, learning_rate, epochs, dropout):\n",
    "    # Инициализация оптимизатора и конфигурация модели\n",
    "    optimizer = nlp.resume_training()\n",
    "    nlp.config[\"training.batch_size\"] = (1.0, 4.0, 1.001)\n",
    "    nlp.config[\"training.dropout\"] = dropout\n",
    "    nlp.config[\"training.epochs\"] = epochs\n",
    "    nlp.config[\"training.learn_rate\"] = learning_rate\n",
    "\n",
    "    # Обучение модели\n",
    "    for i in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        for batch in minibatch(train_data, size = compounding(4.0, 32.0, 1.001)):\n",
    "            texts, annotations = zip(*batch)\n",
    "            examples = []\n",
    "            for text, annot in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annot)\n",
    "                examples.append(example)\n",
    "            nlp.update(examples, drop=dropout, losses=losses, sgd=optimizer)\n",
    "        print(f\"Epoch {i+1}: Losses: {losses}\")\n",
    "\n",
    "    # Оценка модели на тестовом наборе данных\n",
    "    precision, recall, f1 = evaluate_ner_model(nlp, test_data)\n",
    "    print(f\"Losses ner: {losses['ner']} , Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "\n",
    "def evaluate_ner_model(nlp, test_data):\n",
    "    examples = []\n",
    "    for text, annots in test_data:   # assuming evaluation_data is a list of tuples\n",
    "      doc = nlp.make_doc(text)\n",
    "      example = Example.from_dict(doc, annots)\n",
    "      examples.append(example)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scorer = nlp.evaluate(examples)\n",
    "\n",
    "    # Output the evaluation results\n",
    "    precision, recall, f1 = scorer['ents_p'], scorer['ents_r'], scorer['ents_f']\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsVAs66Zqawn",
    "outputId": "407c110a-4ae4-4ce6-b103-2404ab5ab004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9attnAAoxtJ",
    "outputId": "21c57d2f-228f-4a8b-9b9f-1b351ec0d9a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Losses ner: 275.3049806867576 , Precision: 0.0, Recall: 0.0, F1: 0.0\n",
      "Epoch 2: Losses ner: 211.81220833711308 , Precision: 0.0, Recall: 0.0, F1: 0.0\n",
      "Epoch 3: Losses ner: 170.02267289083437 , Precision: 0.7317073170731707, Recall: 0.6818181818181818, F1: 0.7058823529411764\n",
      "Epoch 4: Losses ner: 125.83287750573923 , Precision: 0.8181818181818182, Recall: 0.6136363636363636, F1: 0.7012987012987013\n",
      "Epoch 5: Losses ner: 102.08125042086834 , Precision: 0.717391304347826, Recall: 0.75, F1: 0.7333333333333332\n",
      "Epoch 6: Losses ner: 86.89022280701545 , Precision: 0.75, Recall: 0.4772727272727273, F1: 0.5833333333333334\n",
      "Epoch 7: Losses ner: 86.18001298979435 , Precision: 0.6976744186046512, Recall: 0.6818181818181818, F1: 0.6896551724137931\n",
      "Epoch 8: Losses ner: 64.10151057433964 , Precision: 0.7297297297297297, Recall: 0.6136363636363636, F1: 0.6666666666666666\n",
      "Epoch 9: Losses ner: 53.78603682238264 , Precision: 0.7428571428571429, Recall: 0.5909090909090909, F1: 0.6582278481012659\n",
      "Epoch 10: Losses ner: 48.187513952824936 , Precision: 0.5681818181818182, Recall: 0.5681818181818182, F1: 0.5681818181818182\n"
     ]
    }
   ],
   "source": [
    "nlp_temp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a new NER pipeline if it doesn't exist\n",
    "if \"ner\" not in nlp_temp.pipe_names:\n",
    "    ner = nlp_temp.create_pipe(\"ner\")\n",
    "    nlp_temp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp_temp.get_pipe(\"ner\")\n",
    "\n",
    "# Add the new label (IT-SKILL) to the NER pipeline\n",
    "ner.add_label(\"IT-SKILL\")\n",
    "\n",
    "train_and_evaluate_ner_model(nlp_temp, train_data, test_data, 0.001, 10, 0.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WraivWfcrX3w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iquF-AQhlIDQ",
    "outputId": "d13f6137-be1b-4253-950b-0b4c9ad20f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with learning rate: 0.001, epochs: 10, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 291.3041690945781}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 205.4076216248542}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 122.63955623357344}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 78.44721069629774}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 66.23588131921704}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 65.36121811750327}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.066954112178934}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 37.96070417549064}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 36.76328991860316}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 23.59237785117875}\n",
      "Losses ner: 23.59237785117875 , Precision: 0.6585365853658537, Recall: 0.84375, F1: 0.7397260273972602\n",
      "Training model with learning rate: 0.001, epochs: 10, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 291.6952328066306}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 231.1010374289989}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 188.6312269375954}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 135.67757931016797}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 110.24375381539105}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 92.35744569950829}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 88.48031277681801}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 76.73893818986672}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 91.82477996920623}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 54.598537174907456}\n",
      "Losses ner: 54.598537174907456 , Precision: 0.7142857142857143, Recall: 0.78125, F1: 0.7462686567164178\n",
      "Training model with learning rate: 0.001, epochs: 10, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 304.9892069477138}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 249.08375312537675}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 213.7843059333531}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 175.31198520397774}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 138.47030411408832}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 134.03817079864973}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 126.27816334458524}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 122.28339829169211}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 110.17370027490588}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 105.9478935753983}\n",
      "Losses ner: 105.9478935753983 , Precision: 0.6666666666666666, Recall: 0.75, F1: 0.7058823529411765\n",
      "Training model with learning rate: 0.001, epochs: 20, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 291.302104085585}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 212.00860564928755}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 126.50370878364477}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 89.01171662181056}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 82.97186436671443}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 63.2805012739316}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 41.401493534963855}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 34.45905463487201}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 34.507944896379136}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 30.621486745174117}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 18.362798306619837}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 16.24117365095874}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.176941612580661}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.145765304286751}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.9226486441400175}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.793326137881886}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.844650691815176}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.8692718685256206}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.8221739718063884}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.6406994776058026}\n",
      "Losses ner: 2.6406994776058026 , Precision: 0.7567567567567568, Recall: 0.875, F1: 0.8115942028985507\n",
      "Training model with learning rate: 0.001, epochs: 20, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 284.98663924587254}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 233.54503782629888}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 176.69858426275607}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 129.64943697559573}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 104.45149578249018}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 89.21678851062012}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 85.23498106485562}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 75.45786426964074}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 62.698577979709576}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 54.07930933095036}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 59.37630841847205}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 42.60814546187214}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 41.68773672276714}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 33.9880333150241}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 35.65311991165254}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 28.39229992298513}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 27.10809836977225}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22.999921717346975}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 24.493399350917038}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 17.56266456466084}\n",
      "Losses ner: 17.56266456466084 , Precision: 0.7297297297297297, Recall: 0.84375, F1: 0.7826086956521738\n",
      "Training model with learning rate: 0.001, epochs: 20, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 305.1724332626377}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 252.38561169073324}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 222.2590619953201}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 186.42813586673074}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 164.25103044518934}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 143.00560700994077}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 133.31676638664015}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 123.93417373989008}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 117.78212070961213}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 97.75161092930756}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 98.86171454339924}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 76.91160682929275}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 87.13005961010023}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 75.7768801518373}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 66.62614147377157}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 69.55071939870163}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 50.41036731672342}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 54.7807661578252}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.89259752498718}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 49.393770715730916}\n",
      "Losses ner: 49.393770715730916 , Precision: 0.7567567567567568, Recall: 0.875, F1: 0.8115942028985507\n",
      "Training model with learning rate: 0.001, epochs: 30, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 301.29239979523595}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 204.79882352536873}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 124.54439495373992}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 84.90317469719133}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 81.94205907192571}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 63.69621347507842}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 45.57741719937589}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 33.60301721244411}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 30.608782450972175}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 23.98984400256383}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 21.573614137691624}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 18.364105110703633}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.268417132956149}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.85713892122063}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.390168055047984}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.387462183894453}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 8.110410833652997}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 8.08779586655683}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.68177619519395}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.349962214322399}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.9667542006554837}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.4639899406423273}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.4050923695623507}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.9098337451559413}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.159388294938778}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.2249967647214006}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.6024355726812742}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.9170571858927072}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.2137631289309703}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.014159264611540712}\n",
      "Losses ner: 0.014159264611540712 , Precision: 0.8333333333333334, Recall: 0.9375, F1: 0.8823529411764706\n",
      "Training model with learning rate: 0.001, epochs: 30, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 305.7843269090854}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 227.40084024230504}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 168.8794517592725}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 138.7454826629329}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 114.14875191530649}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 94.99074366330916}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 91.86798587023853}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 94.26214186232508}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 64.55867344224711}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.461095936997204}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 44.0202824231837}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 49.92958744359964}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 39.98845335373511}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 41.4828186593661}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 33.22507228477966}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 37.20480836430622}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 28.35130185111656}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.070602004001938}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 24.714386523411463}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 25.670139737754457}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 20.49751222434564}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 29.921965330532956}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 18.06465885782515}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 11.377508804576003}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.32139103148452}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.840121783921976}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.8796365178951495}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 15.019825319397537}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 13.860097785050042}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.9944410772372605}\n",
      "Losses ner: 0.9944410772372605 , Precision: 0.7435897435897436, Recall: 0.90625, F1: 0.8169014084507042\n",
      "Training model with learning rate: 0.001, epochs: 30, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 294.80230004808215}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 245.59195925608034}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 220.87867971708926}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 173.6833316168175}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 150.5674083850301}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 146.97595368725717}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 122.44617444930034}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 105.26516676732717}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 95.141295885692}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 100.99999820180884}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 102.30110234529845}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.74413962790203}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 79.42416470382491}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 87.84366916442325}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 78.65667720865889}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 69.77516201824275}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.9214058126909}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 69.48402486798219}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 60.68431516828132}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 69.41402599925593}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.83221673642812}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.72294739402658}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.27233063557453}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 41.07306132551295}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 44.87175094863748}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 47.02193647569458}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 45.51640968041509}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 36.239112209373026}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 36.79017355300826}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 38.35488863786699}\n",
      "Losses ner: 38.35488863786699 , Precision: 0.7435897435897436, Recall: 0.90625, F1: 0.8169014084507042\n",
      "Training model with learning rate: 0.01, epochs: 10, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 302.47477351312557}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 202.8804709257161}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 130.1025747745831}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 85.80915171884466}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 89.24099072195925}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 57.51020860661697}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 36.68897769243865}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 25.355156414656957}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.0385607420866}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 26.754641504075842}\n",
      "Losses ner: 26.754641504075842 , Precision: 0.7777777777777778, Recall: 0.875, F1: 0.823529411764706\n",
      "Training model with learning rate: 0.01, epochs: 10, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 294.9003709343311}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 229.77309218152809}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 171.98651256315856}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 130.87972043079517}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.1491260556902}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 98.77413131208064}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 93.89548976284217}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 84.03852799950195}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 69.21051943744607}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 49.69805729685197}\n",
      "Losses ner: 49.69805729685197 , Precision: 0.7105263157894737, Recall: 0.84375, F1: 0.7714285714285714\n",
      "Training model with learning rate: 0.01, epochs: 10, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 306.94225771419605}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 249.11785134755473}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 221.49696399035295}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 183.03998021452824}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 149.16774373985902}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 130.7845044783826}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 123.77439737947488}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 125.46465512079462}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 112.76475879107683}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 109.76013032322962}\n",
      "Losses ner: 109.76013032322962 , Precision: 0.6666666666666666, Recall: 0.75, F1: 0.7058823529411765\n",
      "Training model with learning rate: 0.01, epochs: 20, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 287.62913132666336}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 199.2257853608424}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 113.25764596880703}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 98.92696555155051}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 77.98692183145387}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 72.78500451073344}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 38.716739813802505}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 41.37208642834802}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.794413140615273}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22.16380235818563}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 18.99804689961227}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22.611232089844876}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 15.311817353973641}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.429332310757915}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.910479509277758}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.648335741317522}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.444308777097297}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.3669939172648515}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.001651062888348}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.621378931562034}\n",
      "Losses ner: 1.621378931562034 , Precision: 0.8055555555555556, Recall: 0.90625, F1: 0.8529411764705882\n",
      "Training model with learning rate: 0.01, epochs: 20, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 286.85062303619486}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 227.95328265467538}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 162.8687112393148}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 141.5392386240035}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 107.57571535068276}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 94.22968162953771}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 103.39860418581819}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 98.50027172421173}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 58.42435052338472}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 61.41583873921619}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 55.62687009456159}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 38.38675963230902}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 45.55722036136787}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 38.29418147674306}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 37.67133994396479}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 40.68150662628368}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.420072744224576}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 21.52066193715019}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 25.42268887175319}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.027601792674577}\n",
      "Losses ner: 31.027601792674577 , Precision: 0.7368421052631579, Recall: 0.875, F1: 0.7999999999999999\n",
      "Training model with learning rate: 0.01, epochs: 20, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 311.331162893783}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 241.0724587828242}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 211.9852018995815}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 182.73595731667925}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 150.30881208750503}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 137.3809900827163}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 134.85002833348517}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 118.30712950867066}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 104.28540261457829}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 93.43009197111786}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 99.97252653381935}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 91.68661815585725}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 77.33153168552614}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 72.37254364482033}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 66.58530232717484}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 73.90753929587096}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 61.82408642002294}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 65.41028895748316}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 62.13059960308174}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 60.465908131956844}\n",
      "Losses ner: 60.465908131956844 , Precision: 0.7567567567567568, Recall: 0.875, F1: 0.8115942028985507\n",
      "Training model with learning rate: 0.01, epochs: 30, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 293.1488109264012}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 204.0689530410236}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 123.82170958966144}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 91.39914512034169}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 76.89136437634511}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 59.10974287262127}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.36563839238717}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 39.961439196826085}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 24.843322510560302}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 33.53838046412274}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 18.233424167155228}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 8.864749625930413}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.26203778552277}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 8.951598563142765}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.1291587179237}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.210129742893114}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 11.872946178882275}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.11095289459722}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.9396855436146759}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.560892330872204}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.664887167953128}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.4877710840566557}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.2520528574413556}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.18632363330641538}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.379922747160029}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.033567535798042}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.622982572582111}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.49516321111326445}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0012802119866895318}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.12079310705664421}\n",
      "Losses ner: 0.12079310705664421 , Precision: 0.75, Recall: 0.9375, F1: 0.8333333333333334\n",
      "Training model with learning rate: 0.01, epochs: 30, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 280.1785055100718}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 223.42159602939043}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 167.90992973829626}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 125.06523461563843}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 104.29521038337094}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 88.48035703609142}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 79.44198013898853}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 85.68276976366211}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 69.92811685599384}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 46.93552891069937}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 50.20396451109867}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 46.08648591978158}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 39.27949888090562}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 37.93120407069927}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 29.68576356653196}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.58160059835224}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 33.94121572634029}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 29.350537177919147}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 25.673133278368642}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 26.54237328738746}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22.541834764737985}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 13.619895801323596}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 27.450569713706827}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 11.857725108920228}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.6379735271015825}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.329321250818445}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.090822337763516}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.660350855502474}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 13.596334038701892}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.752979114583377}\n",
      "Losses ner: 9.752979114583377 , Precision: 0.7368421052631579, Recall: 0.875, F1: 0.7999999999999999\n",
      "Training model with learning rate: 0.01, epochs: 30, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 304.95407182800705}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 243.0863222676308}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 199.93451626658788}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 172.90003734857538}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 137.7022853662753}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 125.07163475390777}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 118.75175348775689}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 109.33431565053138}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 119.12961673379178}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 107.84094126466428}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 104.89480355477284}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 82.01425258681142}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 90.13123773877004}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 77.42066059205742}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 60.577512799332716}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 61.24307445736575}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 71.81503266337802}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.3651888365698}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.313838883954844}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 49.22121464488283}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 59.47619617672747}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 42.97850977156843}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 47.350834204572166}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.3180364586637}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 46.16607464908903}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 29.047125557107584}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.78787269710505}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 33.99822773605322}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 45.532977147556586}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 39.344488524898054}\n",
      "Losses ner: 39.344488524898054 , Precision: 0.7631578947368421, Recall: 0.90625, F1: 0.8285714285714286\n",
      "Training model with learning rate: 0.1, epochs: 10, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 285.28833667404786}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 210.96814815386603}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 133.60042931599838}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 94.65580047867283}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 74.55682047845677}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 80.90754526773526}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 50.95011711434612}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 36.01249751262992}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 25.71987678206986}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 30.293171514471656}\n",
      "Losses ner: 30.293171514471656 , Precision: 0.7419354838709677, Recall: 0.71875, F1: 0.7301587301587302\n",
      "Training model with learning rate: 0.1, epochs: 10, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 298.38980313496563}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 229.1548793656156}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 184.76276287997098}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 123.40335885842107}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 105.03972511315283}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 86.9237751924492}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 87.09778409697093}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 73.69471430789149}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 73.73571321650522}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 55.822972653633315}\n",
      "Losses ner: 55.822972653633315 , Precision: 0.7142857142857143, Recall: 0.78125, F1: 0.7462686567164178\n",
      "Training model with learning rate: 0.1, epochs: 10, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 298.00699766348026}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 245.26665105972708}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 215.47189644519892}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 184.22415390386664}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 160.6063402759996}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 148.10545678219717}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 121.01635667999643}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 132.75774066061734}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 122.71425865361901}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 98.36015342041266}\n",
      "Losses ner: 98.36015342041266 , Precision: 0.6578947368421053, Recall: 0.78125, F1: 0.7142857142857143\n",
      "Training model with learning rate: 0.1, epochs: 20, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 293.3449480354516}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 206.36466777895032}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 125.44141482088153}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 79.63179245930368}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 84.73246254486722}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 62.895948886283904}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 47.27751426305628}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.75963191029821}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 21.145538796308564}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 25.809697691910927}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 18.807561957934876}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 17.67493480644726}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 11.467025654304932}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.98673838813918}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.378478817531946}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.456086300375403}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.577512237151859}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.25768228365672874}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.8681092929857197}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.241884463326228}\n",
      "Losses ner: 9.241884463326228 , Precision: 0.7105263157894737, Recall: 0.84375, F1: 0.7714285714285714\n",
      "Training model with learning rate: 0.1, epochs: 20, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 303.6321996956965}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 229.69893088057702}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 185.63786717697656}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 123.99502593308117}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 116.15429416013714}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.25040924115393}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 84.41161097019092}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 95.74161921377979}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.318447436401875}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.29757121326671}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 41.33948915257412}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 43.06412583262931}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 41.11268249571173}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 35.46075182376197}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 27.456790408986805}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 20.565907499547}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 20.700097146730148}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 17.80435657905926}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 21.51680967200608}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 28.123069268456803}\n",
      "Losses ner: 28.123069268456803 , Precision: 0.7631578947368421, Recall: 0.90625, F1: 0.8285714285714286\n",
      "Training model with learning rate: 0.1, epochs: 20, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 303.8875208303573}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 242.63090644415854}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 212.56457825940726}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 182.414831831033}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 151.9278310323102}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 132.15687953880052}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 119.4760892408369}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 124.71742660484294}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 99.7080628079137}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.46716342703952}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 104.40910664500596}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 91.77909810679483}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 70.8274340347408}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 67.67535804643465}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 77.87702101045292}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 60.71063226304398}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 75.47509106796323}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 53.05317466143967}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.433362371532624}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 54.317579519586516}\n",
      "Losses ner: 54.317579519586516 , Precision: 0.7837837837837838, Recall: 0.90625, F1: 0.8405797101449275\n",
      "Training model with learning rate: 0.1, epochs: 30, dropout: 0.2\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 297.27139104425464}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 208.02770181325832}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 121.28699742364932}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 83.80734457359489}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 78.34733350162358}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 67.00199624544348}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 63.65746155339821}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 36.92469408468048}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 34.33929152423687}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 23.56455733225315}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 26.829938428367697}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.592731053157873}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.648522840089369}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.475685865392501}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 8.730432824743657}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.90791720130791}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.813501787596495}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.576770828555931}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.404884305716758}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.233218456908705}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.45447751496792}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.343944682343185}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.011561816834970979}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.4235811067498516}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.0743618349871977}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.2121777227113855}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.970646869641317}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.1858192447757046}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.006055337476216623}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.192069556128405}\n",
      "Losses ner: 3.192069556128405 , Precision: 0.7567567567567568, Recall: 0.875, F1: 0.8115942028985507\n",
      "Training model with learning rate: 0.1, epochs: 30, dropout: 0.35\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 293.86590776910685}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 225.59000896647484}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 177.63253394064316}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 121.5620170694586}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 94.82647209950125}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 89.76061465662714}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 93.77714210192632}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 68.91913259675147}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 61.466970301868656}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 59.5475672431364}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 50.74755364557721}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 45.71769610805049}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 42.76505486052592}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 47.904328217133845}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 32.680887373008936}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 37.82176343805946}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22.485418223598824}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 15.629751423265526}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 26.14284387844537}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 14.074743755618176}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 14.615178091254771}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.610774256908586}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 16.80365325603751}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 14.721938945352855}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 16.125633769726054}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 14.577301825034798}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.410676843147673}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.46361487037695}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.8352244359428065}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 16.00121636690611}\n",
      "Losses ner: 16.00121636690611 , Precision: 0.7567567567567568, Recall: 0.875, F1: 0.8115942028985507\n",
      "Training model with learning rate: 0.1, epochs: 30, dropout: 0.5\n",
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 319.8941956949715}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 248.06127921617545}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 225.9400780736035}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 187.45285751731473}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 165.63252466395568}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 145.54841791791588}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 121.92880680827828}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 129.06926741849662}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 115.55221895520273}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 103.2343197903439}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 95.89997406075648}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.12199680101295}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 80.18483817126402}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 71.0099716979306}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 71.02585257372918}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 65.3449442866251}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 58.45453944731851}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 54.567726197162855}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 64.39658857624522}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.1453478873026}\n",
      "Epoch 21: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 50.18573678453103}\n",
      "Epoch 22: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 59.5164170049178}\n",
      "Epoch 23: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 46.63725072758761}\n",
      "Epoch 24: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 42.90779649012208}\n",
      "Epoch 25: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 58.08612316423021}\n",
      "Epoch 26: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 42.20532521893182}\n",
      "Epoch 27: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 40.16723684675735}\n",
      "Epoch 28: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 43.26521726381243}\n",
      "Epoch 29: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 37.11903651063492}\n",
      "Epoch 30: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22.92107340294974}\n",
      "Losses ner: 22.92107340294974 , Precision: 0.7, Recall: 0.875, F1: 0.7777777777777777\n",
      "Scores:\n",
      "Learning rate: 0.001, Epochs: 10, Dropout: 0.2 - F1: 0.7397260273972602\n",
      "Learning rate: 0.001, Epochs: 10, Dropout: 0.35 - F1: 0.7462686567164178\n",
      "Learning rate: 0.001, Epochs: 10, Dropout: 0.5 - F1: 0.7058823529411765\n",
      "Learning rate: 0.001, Epochs: 20, Dropout: 0.2 - F1: 0.8115942028985507\n",
      "Learning rate: 0.001, Epochs: 20, Dropout: 0.35 - F1: 0.7826086956521738\n",
      "Learning rate: 0.001, Epochs: 20, Dropout: 0.5 - F1: 0.8115942028985507\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.2 - F1: 0.8823529411764706\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.35 - F1: 0.8169014084507042\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.5 - F1: 0.8169014084507042\n",
      "Learning rate: 0.01, Epochs: 10, Dropout: 0.2 - F1: 0.823529411764706\n",
      "Learning rate: 0.01, Epochs: 10, Dropout: 0.35 - F1: 0.7714285714285714\n",
      "Learning rate: 0.01, Epochs: 10, Dropout: 0.5 - F1: 0.7058823529411765\n",
      "Learning rate: 0.01, Epochs: 20, Dropout: 0.2 - F1: 0.8529411764705882\n",
      "Learning rate: 0.01, Epochs: 20, Dropout: 0.35 - F1: 0.7999999999999999\n",
      "Learning rate: 0.01, Epochs: 20, Dropout: 0.5 - F1: 0.8115942028985507\n",
      "Learning rate: 0.01, Epochs: 30, Dropout: 0.2 - F1: 0.8333333333333334\n",
      "Learning rate: 0.01, Epochs: 30, Dropout: 0.35 - F1: 0.7999999999999999\n",
      "Learning rate: 0.01, Epochs: 30, Dropout: 0.5 - F1: 0.8285714285714286\n",
      "Learning rate: 0.1, Epochs: 10, Dropout: 0.2 - F1: 0.7301587301587302\n",
      "Learning rate: 0.1, Epochs: 10, Dropout: 0.35 - F1: 0.7462686567164178\n",
      "Learning rate: 0.1, Epochs: 10, Dropout: 0.5 - F1: 0.7142857142857143\n",
      "Learning rate: 0.1, Epochs: 20, Dropout: 0.2 - F1: 0.7714285714285714\n",
      "Learning rate: 0.1, Epochs: 20, Dropout: 0.35 - F1: 0.8285714285714286\n",
      "Learning rate: 0.1, Epochs: 20, Dropout: 0.5 - F1: 0.8405797101449275\n",
      "Learning rate: 0.1, Epochs: 30, Dropout: 0.2 - F1: 0.8115942028985507\n",
      "Learning rate: 0.1, Epochs: 30, Dropout: 0.35 - F1: 0.8115942028985507\n",
      "Learning rate: 0.1, Epochs: 30, Dropout: 0.5 - F1: 0.7777777777777777\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Загрузка модели и подготовка данных\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Разделение TRAIN_DATA на тренировочный и тестовый наборы (80% на 20%)\n",
    "random.shuffle(TRAIN_DATA)\n",
    "train_size = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:train_size]\n",
    "test_data = TRAIN_DATA[train_size:]\n",
    "\n",
    "# Параметры обучения\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "epochs = [10, 20, 30]\n",
    "dropouts = [0.2, 0.35, 0.5]\n",
    "\n",
    "model_name = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Обучение и оценка моделей с разными параметрами\n",
    "results = np.zeros((len(learning_rates), len(epochs), len(dropouts)))\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, epoch in enumerate(epochs):\n",
    "        for k, dropout in enumerate(dropouts):\n",
    "            print(f\"Training model with learning rate: {lr}, epochs: {epoch}, dropout: {dropout}\")\n",
    "\n",
    "            # Создание новой модели для каждой комбинации параметров\n",
    "            nlp_temp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "            #Добавление нового NER пайплайн если не существует\n",
    "            if \"ner\" not in nlp_temp.pipe_names:\n",
    "                ner = nlp_temp.create_pipe(\"ner\")\n",
    "                nlp_temp.add_pipe(\"ner\", last=True)\n",
    "            else:\n",
    "                ner = nlp_temp.get_pipe(\"ner\")\n",
    "\n",
    "            # Добавление лейбла IT-SKILL для пайплайн NER\n",
    "            ner.add_label(\"IT-SKILL\")\n",
    "            \n",
    "            train_and_evaluate_ner_model(nlp_temp, train_data, test_data, lr, epoch, dropout)\n",
    "\n",
    "            # Оценка модели и сохранение результатов\n",
    "            precision, recall, f1 = evaluate_ner_model(nlp_temp, test_data)\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)\n",
    "\n",
    "            results[i, j, k] = f1\n",
    "\n",
    "\n",
    "print(\"Scores:\")\n",
    "for i, lr in enumerate(learning_rates):\n",
    "  for j, epoch in enumerate(epochs):\n",
    "    for k, dropout in enumerate(dropouts):\n",
    "      print(f\"Learning rate: {lr}, Epochs: {epoch}, Dropout: {dropout} - F1: {results[i, j, k]}\")\n",
    "      model_name.append(f\"ner_model_lr{lr}_ep{epoch}_dr{dropout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 896
    },
    "id": "9xLNRnYawB1t",
    "outputId": "8139aebf-726f-489c-ec18-297cc1c820a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-88583e2f-2d60-425b-baa8-dbdadecba7e7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ner_model_lr0.001_ep10_dr0.2</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ner_model_lr0.001_ep10_dr0.35</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ner_model_lr0.001_ep10_dr0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ner_model_lr0.001_ep20_dr0.2</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ner_model_lr0.001_ep20_dr0.35</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ner_model_lr0.001_ep20_dr0.5</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ner_model_lr0.001_ep30_dr0.2</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ner_model_lr0.001_ep30_dr0.35</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ner_model_lr0.001_ep30_dr0.5</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ner_model_lr0.01_ep10_dr0.2</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ner_model_lr0.01_ep10_dr0.35</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ner_model_lr0.01_ep10_dr0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ner_model_lr0.01_ep20_dr0.2</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ner_model_lr0.01_ep20_dr0.35</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ner_model_lr0.01_ep20_dr0.5</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ner_model_lr0.01_ep30_dr0.2</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ner_model_lr0.01_ep30_dr0.35</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ner_model_lr0.01_ep30_dr0.5</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ner_model_lr0.1_ep10_dr0.2</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.730159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ner_model_lr0.1_ep10_dr0.35</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ner_model_lr0.1_ep10_dr0.5</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ner_model_lr0.1_ep20_dr0.2</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ner_model_lr0.1_ep20_dr0.35</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ner_model_lr0.1_ep20_dr0.5</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.840580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ner_model_lr0.1_ep30_dr0.2</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ner_model_lr0.1_ep30_dr0.35</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ner_model_lr0.1_ep30_dr0.5</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88583e2f-2d60-425b-baa8-dbdadecba7e7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-88583e2f-2d60-425b-baa8-dbdadecba7e7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-88583e2f-2d60-425b-baa8-dbdadecba7e7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                        NER Model  Precision   Recall        F1\n",
       "0    ner_model_lr0.001_ep10_dr0.2   0.658537  0.84375  0.739726\n",
       "1   ner_model_lr0.001_ep10_dr0.35   0.714286  0.78125  0.746269\n",
       "2    ner_model_lr0.001_ep10_dr0.5   0.666667  0.75000  0.705882\n",
       "3    ner_model_lr0.001_ep20_dr0.2   0.756757  0.87500  0.811594\n",
       "4   ner_model_lr0.001_ep20_dr0.35   0.729730  0.84375  0.782609\n",
       "5    ner_model_lr0.001_ep20_dr0.5   0.756757  0.87500  0.811594\n",
       "6    ner_model_lr0.001_ep30_dr0.2   0.833333  0.93750  0.882353\n",
       "7   ner_model_lr0.001_ep30_dr0.35   0.743590  0.90625  0.816901\n",
       "8    ner_model_lr0.001_ep30_dr0.5   0.743590  0.90625  0.816901\n",
       "9     ner_model_lr0.01_ep10_dr0.2   0.777778  0.87500  0.823529\n",
       "10   ner_model_lr0.01_ep10_dr0.35   0.710526  0.84375  0.771429\n",
       "11    ner_model_lr0.01_ep10_dr0.5   0.666667  0.75000  0.705882\n",
       "12    ner_model_lr0.01_ep20_dr0.2   0.805556  0.90625  0.852941\n",
       "13   ner_model_lr0.01_ep20_dr0.35   0.736842  0.87500  0.800000\n",
       "14    ner_model_lr0.01_ep20_dr0.5   0.756757  0.87500  0.811594\n",
       "15    ner_model_lr0.01_ep30_dr0.2   0.750000  0.93750  0.833333\n",
       "16   ner_model_lr0.01_ep30_dr0.35   0.736842  0.87500  0.800000\n",
       "17    ner_model_lr0.01_ep30_dr0.5   0.763158  0.90625  0.828571\n",
       "18     ner_model_lr0.1_ep10_dr0.2   0.741935  0.71875  0.730159\n",
       "19    ner_model_lr0.1_ep10_dr0.35   0.714286  0.78125  0.746269\n",
       "20     ner_model_lr0.1_ep10_dr0.5   0.657895  0.78125  0.714286\n",
       "21     ner_model_lr0.1_ep20_dr0.2   0.710526  0.84375  0.771429\n",
       "22    ner_model_lr0.1_ep20_dr0.35   0.763158  0.90625  0.828571\n",
       "23     ner_model_lr0.1_ep20_dr0.5   0.783784  0.90625  0.840580\n",
       "24     ner_model_lr0.1_ep30_dr0.2   0.756757  0.87500  0.811594\n",
       "25    ner_model_lr0.1_ep30_dr0.35   0.756757  0.87500  0.811594\n",
       "26     ner_model_lr0.1_ep30_dr0.5   0.700000  0.87500  0.777778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Создание DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'NER Model': model_name,\n",
    "    'Precision': precision_list,\n",
    "    'Recall': recall_list,\n",
    "    'F1': f1_list\n",
    "})\n",
    "\n",
    "# Вывод таблицы\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 896
    },
    "id": "Yp4uSREP956h",
    "outputId": "d397ad18-642d-4417-bfe5-bcbb3fba66a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f41bb436-61f3-4b9f-94b9-801562a974b9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ner_model_lr0.001_ep30_dr0.2</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ner_model_lr0.01_ep20_dr0.2</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ner_model_lr0.1_ep20_dr0.5</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.840580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ner_model_lr0.01_ep30_dr0.2</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ner_model_lr0.01_ep30_dr0.5</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ner_model_lr0.1_ep20_dr0.35</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ner_model_lr0.01_ep10_dr0.2</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ner_model_lr0.001_ep30_dr0.5</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ner_model_lr0.001_ep30_dr0.35</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ner_model_lr0.01_ep20_dr0.5</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ner_model_lr0.1_ep30_dr0.35</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ner_model_lr0.001_ep20_dr0.2</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ner_model_lr0.001_ep20_dr0.5</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ner_model_lr0.1_ep30_dr0.2</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ner_model_lr0.01_ep20_dr0.35</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ner_model_lr0.01_ep30_dr0.35</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ner_model_lr0.001_ep20_dr0.35</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ner_model_lr0.1_ep30_dr0.5</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ner_model_lr0.1_ep20_dr0.2</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ner_model_lr0.01_ep10_dr0.35</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ner_model_lr0.1_ep10_dr0.35</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ner_model_lr0.001_ep10_dr0.35</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ner_model_lr0.001_ep10_dr0.2</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ner_model_lr0.1_ep10_dr0.2</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.730159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ner_model_lr0.1_ep10_dr0.5</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ner_model_lr0.01_ep10_dr0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ner_model_lr0.001_ep10_dr0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f41bb436-61f3-4b9f-94b9-801562a974b9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f41bb436-61f3-4b9f-94b9-801562a974b9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f41bb436-61f3-4b9f-94b9-801562a974b9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                        NER Model  Precision   Recall        F1\n",
       "6    ner_model_lr0.001_ep30_dr0.2   0.833333  0.93750  0.882353\n",
       "12    ner_model_lr0.01_ep20_dr0.2   0.805556  0.90625  0.852941\n",
       "23     ner_model_lr0.1_ep20_dr0.5   0.783784  0.90625  0.840580\n",
       "15    ner_model_lr0.01_ep30_dr0.2   0.750000  0.93750  0.833333\n",
       "17    ner_model_lr0.01_ep30_dr0.5   0.763158  0.90625  0.828571\n",
       "22    ner_model_lr0.1_ep20_dr0.35   0.763158  0.90625  0.828571\n",
       "9     ner_model_lr0.01_ep10_dr0.2   0.777778  0.87500  0.823529\n",
       "8    ner_model_lr0.001_ep30_dr0.5   0.743590  0.90625  0.816901\n",
       "7   ner_model_lr0.001_ep30_dr0.35   0.743590  0.90625  0.816901\n",
       "14    ner_model_lr0.01_ep20_dr0.5   0.756757  0.87500  0.811594\n",
       "25    ner_model_lr0.1_ep30_dr0.35   0.756757  0.87500  0.811594\n",
       "3    ner_model_lr0.001_ep20_dr0.2   0.756757  0.87500  0.811594\n",
       "5    ner_model_lr0.001_ep20_dr0.5   0.756757  0.87500  0.811594\n",
       "24     ner_model_lr0.1_ep30_dr0.2   0.756757  0.87500  0.811594\n",
       "13   ner_model_lr0.01_ep20_dr0.35   0.736842  0.87500  0.800000\n",
       "16   ner_model_lr0.01_ep30_dr0.35   0.736842  0.87500  0.800000\n",
       "4   ner_model_lr0.001_ep20_dr0.35   0.729730  0.84375  0.782609\n",
       "26     ner_model_lr0.1_ep30_dr0.5   0.700000  0.87500  0.777778\n",
       "21     ner_model_lr0.1_ep20_dr0.2   0.710526  0.84375  0.771429\n",
       "10   ner_model_lr0.01_ep10_dr0.35   0.710526  0.84375  0.771429\n",
       "19    ner_model_lr0.1_ep10_dr0.35   0.714286  0.78125  0.746269\n",
       "1   ner_model_lr0.001_ep10_dr0.35   0.714286  0.78125  0.746269\n",
       "0    ner_model_lr0.001_ep10_dr0.2   0.658537  0.84375  0.739726\n",
       "18     ner_model_lr0.1_ep10_dr0.2   0.741935  0.71875  0.730159\n",
       "20     ner_model_lr0.1_ep10_dr0.5   0.657895  0.78125  0.714286\n",
       "11    ner_model_lr0.01_ep10_dr0.5   0.666667  0.75000  0.705882\n",
       "2    ner_model_lr0.001_ep10_dr0.5   0.666667  0.75000  0.705882"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xoPLCL49uXq"
   },
   "outputs": [],
   "source": [
    "df.to_csv('comparison_ner_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfIw1iXLA9Oa"
   },
   "outputs": [],
   "source": [
    "nlp.to_disk(\"it_skills_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMdRLtq1BuT2",
    "outputId": "810bd059-907b-4e2d-a2f9-a01ade21bb7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('опыт разработки на java;', {'entities': [(15, 19, 'IT-SKILL')]}),\n",
       " ('создал визуализации данных с использованием d3.js и tableau',\n",
       "  {'entities': [(7, 26, 'IT-SKILL'),\n",
       "    (44, 49, 'IT-SKILL'),\n",
       "    (52, 59, 'IT-SKILL')]}),\n",
       " ('implemented cybersecurity measures, including penetration testing and security audits.',\n",
       "  {'entities': [(12, 34, 'IT-SKILL'),\n",
       "    (46, 65, 'IT-SKILL'),\n",
       "    (70, 84, 'IT-SKILL')]}),\n",
       " ('developed applications using react, angular, and vue.js.',\n",
       "  {'entities': [(29, 34, 'IT-SKILL'),\n",
       "    (36, 43, 'IT-SKILL'),\n",
       "    (49, 55, 'IT-SKILL')]}),\n",
       " ('знание языков программирования python и/или java',\n",
       "  {'entities': [(7, 21, 'IT-SKILL'),\n",
       "    (26, 32, 'IT-SKILL'),\n",
       "    (35, 39, 'IT-SKILL')]}),\n",
       " ('experienced in cloud technologies such as aws, azure, and google cloud platform.',\n",
       "  {'entities': [(15, 20, 'IT-SKILL'),\n",
       "    (42, 45, 'IT-SKILL'),\n",
       "    (47, 52, 'IT-SKILL'),\n",
       "    (58, 79, 'IT-SKILL')]}),\n",
       " ('experience with data mining and statistical analysis using r and python.',\n",
       "  {'entities': [(14, 24, 'IT-SKILL'),\n",
       "    (42, 43, 'IT-SKILL'),\n",
       "    (48, 55, 'IT-SKILL'),\n",
       "    (60, 66, 'IT-SKILL')]}),\n",
       " ('experience with mobile application development using react native and swift.',\n",
       "  {'entities': [(14, 38, 'IT-SKILL'), (43, 48, 'IT-SKILL')]}),\n",
       " ('занимался разработкой с использованием git, docker и kubernetes.',\n",
       "  {'entities': [(39, 42, 'IT-SKILL'),\n",
       "    (44, 50, 'IT-SKILL'),\n",
       "    (53, 63, 'IT-SKILL')]}),\n",
       " ('upgraded functionality of distributed caching system based on oracle coherence',\n",
       "  {'entities': [(62, 78, 'IT-SKILL')]}),\n",
       " ('extensive experience with ruby on rails and django.',\n",
       "  {'entities': [(26, 39, 'IT-SKILL'), (44, 50, 'IT-SKILL')]}),\n",
       " ('опыт работы с базами данных mongodb, redis, cassandra;',\n",
       "  {'entities': [(25, 32, 'IT-SKILL'),\n",
       "    (34, 38, 'IT-SKILL'),\n",
       "    (40, 48, 'IT-SKILL')]}),\n",
       " ('developed mobile applications using swift and kotlin for ios and android',\n",
       "  {'entities': [(10, 28, 'IT-SKILL'),\n",
       "    (36, 41, 'IT-SKILL'),\n",
       "    (46, 52, 'IT-SKILL'),\n",
       "    (57, 60, 'IT-SKILL'),\n",
       "    (65, 72, 'IT-SKILL')]}),\n",
       " ('опыт разработки на java ee и spring framework;',\n",
       "  {'entities': [(22, 29, 'IT-SKILL'), (34, 50, 'IT-SKILL')]}),\n",
       " ('работал с серверными технологиями, такими как node.js, express.js и ruby on rails.',\n",
       "  {'entities': [(46, 53, 'IT-SKILL'),\n",
       "    (55, 65, 'IT-SKILL'),\n",
       "    (68, 82, 'IT-SKILL')]}),\n",
       " ('опыт разработки на javascript и фреймворках react и angular;',\n",
       "  {'entities': [(22, 33, 'IT-SKILL'),\n",
       "    (38, 43, 'IT-SKILL'),\n",
       "    (48, 55, 'IT-SKILL')]}),\n",
       " ('умение писать понятный код с комментированием и применением ключевых стандартов, рефакторинг при необходимости;',\n",
       "  {'entities': [(81, 92, 'IT-SKILL')]}),\n",
       " ('плюсом будет знание go и/или с#',\n",
       "  {'entities': [(20, 22, 'IT-SKILL'), (29, 31, 'IT-SKILL')]}),\n",
       " ('знание принципов работы и настройки веб-серверов apache и nginx;',\n",
       "  {'entities': [(0, 16, 'IT-SKILL'),\n",
       "    (19, 25, 'IT-SKILL'),\n",
       "    (30, 41, 'IT-SKILL'),\n",
       "    (44, 49, 'IT-SKILL')]}),\n",
       " ('опыт разработки мобильных приложений под ios и android;',\n",
       "  {'entities': [(14, 31, 'IT-SKILL'),\n",
       "    (36, 43, 'IT-SKILL'),\n",
       "    (48, 55, 'IT-SKILL')]}),\n",
       " ('знание принципов работы операционных систем windows и macos;',\n",
       "  {'entities': [(0, 6, 'IT-SKILL'),\n",
       "    (9, 16, 'IT-SKILL'),\n",
       "    (21, 26, 'IT-SKILL'),\n",
       "    (29, 34, 'IT-SKILL')]}),\n",
       " ('знание принципов функционального программирования',\n",
       "  {'entities': [(7, 14, 'IT-SKILL'), (28, 53, 'IT-SKILL')]}),\n",
       " ('опыт работы с микросервисами и контейнеризацией docker или kubernetes',\n",
       "  {'entities': [(7, 21, 'IT-SKILL'),\n",
       "    (37, 43, 'IT-SKILL'),\n",
       "    (48, 57, 'IT-SKILL')]}),\n",
       " ('у меня есть опыт работы с java, kotlin и swift для мобильной разработки.',\n",
       "  {'entities': [(26, 30, 'IT-SKILL'),\n",
       "    (32, 38, 'IT-SKILL'),\n",
       "    (41, 46, 'IT-SKILL')]}),\n",
       " ('implemented continuous integration and delivery with jenkins and gitlab.',\n",
       "  {'entities': [(12, 39, 'IT-SKILL'),\n",
       "    (44, 50, 'IT-SKILL'),\n",
       "    (55, 61, 'IT-SKILL')]}),\n",
       " ('опыт разработки мобильных приложений на android и/или ios',\n",
       "  {'entities': [(32, 39, 'IT-SKILL'), (44, 51, 'IT-SKILL')]}),\n",
       " ('implemented ci/cd pipeline using jenkins and docker',\n",
       "  {'entities': [(12, 17, 'IT-SKILL'),\n",
       "    (33, 40, 'IT-SKILL'),\n",
       "    (45, 51, 'IT-SKILL')]}),\n",
       " ('умение работать с системами контроля версий svn и mercurial;',\n",
       "  {'entities': [(31, 34, 'IT-SKILL'), (39, 48, 'IT-SKILL')]}),\n",
       " ('used r for data analysis and python for data visualization.',\n",
       "  {'entities': [(5, 6, 'IT-SKILL'),\n",
       "    (11, 24, 'IT-SKILL'),\n",
       "    (29, 35, 'IT-SKILL'),\n",
       "    (40, 58, 'IT-SKILL')]}),\n",
       " ('experience with agile software development and project management methodologies.',\n",
       "  {'entities': [(14, 19, 'IT-SKILL'),\n",
       "    (37, 44, 'IT-SKILL'),\n",
       "    (49, 68, 'IT-SKILL')]}),\n",
       " ('понимание soap, rest;',\n",
       "  {'entities': [(10, 14, 'IT-SKILL'), (16, 20, 'IT-SKILL')]}),\n",
       " ('ответственность, самоорганизованность, коммуникабельность.',\n",
       "  {'entities': [(17, 37, 'IT-SKILL'), (39, 57, 'IT-SKILL')]}),\n",
       " ('понимание restapi, openapi (например swagger), kafka;',\n",
       "  {'entities': [(10, 17, 'IT-SKILL'),\n",
       "    (19, 26, 'IT-SKILL'),\n",
       "    (37, 44, 'IT-SKILL'),\n",
       "    (47, 52, 'IT-SKILL')]}),\n",
       " ('использовал react, angular и vue.js для создания пользовательских интерфейсов.',\n",
       "  {'entities': [(12, 17, 'IT-SKILL'),\n",
       "    (19, 26, 'IT-SKILL'),\n",
       "    (29, 35, 'IT-SKILL')]}),\n",
       " ('знание техник тест-дизайна;', {'entities': [(14, 25, 'IT-SKILL')]}),\n",
       " ('опыт администрирования linux;', {'entities': [(23, 28, 'IT-SKILL')]}),\n",
       " ('опыт работы с системами контроля версий, такими как mercurial;',\n",
       "  {'entities': [(13, 29, 'IT-SKILL'), (52, 62, 'IT-SKILL')]}),\n",
       " ('experience with network and server administration, including dns and tcp/ip.',\n",
       "  {'entities': [(14, 21, 'IT-SKILL'),\n",
       "    (26, 32, 'IT-SKILL'),\n",
       "    (53, 62, 'IT-SKILL'),\n",
       "    (67, 72, 'IT-SKILL')]}),\n",
       " ('used agile methodologies, scrum and kanban, for project management',\n",
       "  {'entities': [(5, 10, 'IT-SKILL'),\n",
       "    (23, 28, 'IT-SKILL'),\n",
       "    (33, 39, 'IT-SKILL')]}),\n",
       " ('умение работать с linux-серверами и настройкой сетей;',\n",
       "  {'entities': [(15, 28, 'IT-SKILL'), (32, 40, 'IT-SKILL')]}),\n",
       " (\"i'm proficient in html, css, and javascript for web development.\",\n",
       "  {'entities': [(18, 22, 'IT-SKILL'),\n",
       "    (24, 27, 'IT-SKILL'),\n",
       "    (33, 43, 'IT-SKILL'),\n",
       "    (48, 63, 'IT-SKILL')]}),\n",
       " ('implemented machine learning models using tensorflow and keras.',\n",
       "  {'entities': [(12, 28, 'IT-SKILL'),\n",
       "    (42, 52, 'IT-SKILL'),\n",
       "    (57, 62, 'IT-SKILL')]}),\n",
       " ('хорошие знания php, laravel и node.js',\n",
       "  {'entities': [(15, 18, 'IT-SKILL'),\n",
       "    (20, 27, 'IT-SKILL'),\n",
       "    (30, 37, 'IT-SKILL')]}),\n",
       " ('built restful apis with node.js and express.js.',\n",
       "  {'entities': [(6, 18, 'IT-SKILL'),\n",
       "    (24, 31, 'IT-SKILL'),\n",
       "    (36, 46, 'IT-SKILL')]}),\n",
       " ('уверенное владение python и библиотеками для машинного обучения (например, scikit-learn, tensorflow, keras);',\n",
       "  {'entities': [(17, 23, 'IT-SKILL'),\n",
       "    (43, 56, 'IT-SKILL'),\n",
       "    (58, 68, 'IT-SKILL'),\n",
       "    (70, 76, 'IT-SKILL')]}),\n",
       " ('специализировался на обработке данных с помощью tensorflow, keras и pytorch.',\n",
       "  {'entities': [(48, 58, 'IT-SKILL'),\n",
       "    (60, 65, 'IT-SKILL'),\n",
       "    (68, 75, 'IT-SKILL')]}),\n",
       " ('я занимался анализом данных с использованием r, pandas и matplotlib.',\n",
       "  {'entities': [(45, 46, 'IT-SKILL'),\n",
       "    (48, 54, 'IT-SKILL'),\n",
       "    (57, 67, 'IT-SKILL')]}),\n",
       " ('настроил и поддерживал облачную инфраструктуру на aws, gcp и azure',\n",
       "  {'entities': [(23, 46, 'IT-SKILL'),\n",
       "    (50, 53, 'IT-SKILL'),\n",
       "    (55, 58, 'IT-SKILL'),\n",
       "    (61, 66, 'IT-SKILL')]}),\n",
       " (\"developed a new api using a new framework (tomcat/spring/maven/git) to replace legacy the api (jboss/struts/ejb/svn) as part of company's migration.\",\n",
       "  {'entities': [(16, 19, 'IT-SKILL'),\n",
       "    (43, 49, 'IT-SKILL'),\n",
       "    (50, 56, 'IT-SKILL'),\n",
       "    (57, 62, 'IT-SKILL'),\n",
       "    (63, 66, 'IT-SKILL'),\n",
       "    (95, 100, 'IT-SKILL'),\n",
       "    (101, 107, 'IT-SKILL'),\n",
       "    (108, 111, 'IT-SKILL'),\n",
       "    (112, 115, 'IT-SKILL')]}),\n",
       " ('знание основных протоколов сетевого взаимодействия (http, ftp, smtp, dns);',\n",
       "  {'entities': [(0, 6, 'IT-SKILL'),\n",
       "    (8, 11, 'IT-SKILL'),\n",
       "    (13, 17, 'IT-SKILL'),\n",
       "    (19, 23, 'IT-SKILL'),\n",
       "    (25, 28, 'IT-SKILL'),\n",
       "    (30, 33, 'IT-SKILL')]}),\n",
       " ('full stack web developer with expertise in python, django, and react.js',\n",
       "  {'entities': [(0, 10, 'IT-SKILL'),\n",
       "    (11, 24, 'IT-SKILL'),\n",
       "    (43, 49, 'IT-SKILL'),\n",
       "    (51, 57, 'IT-SKILL'),\n",
       "    (63, 71, 'IT-SKILL')]}),\n",
       " ('created data visualizations using d3.js and tableau',\n",
       "  {'entities': [(8, 27, 'IT-SKILL'),\n",
       "    (34, 39, 'IT-SKILL'),\n",
       "    (44, 51, 'IT-SKILL')]}),\n",
       " ('умение работать с платформой amazon web services (aws);',\n",
       "  {'entities': [(15, 36, 'IT-SKILL'),\n",
       "    (38, 41, 'IT-SKILL'),\n",
       "    (43, 49, 'IT-SKILL')]}),\n",
       " ('implemented machine learning algorithms using tensorflow and keras',\n",
       "  {'entities': [(12, 28, 'IT-SKILL'),\n",
       "    (46, 56, 'IT-SKILL'),\n",
       "    (61, 66, 'IT-SKILL')]}),\n",
       " ('понимание html, css;',\n",
       "  {'entities': [(10, 14, 'IT-SKILL'), (16, 19, 'IT-SKILL')]}),\n",
       " ('опыт работы с базами данных mysql, postgresql;',\n",
       "  {'entities': [(28, 33, 'IT-SKILL'), (35, 46, 'IT-SKILL')]}),\n",
       " ('worked with database management systems like mysql and postgresql.',\n",
       "  {'entities': [(45, 50, 'IT-SKILL'), (55, 65, 'IT-SKILL')]}),\n",
       " ('знания sql (postgresql), ci/cd, docker, kubernetes;',\n",
       "  {'entities': [(7, 10, 'IT-SKILL'),\n",
       "    (12, 22, 'IT-SKILL'),\n",
       "    (25, 30, 'IT-SKILL'),\n",
       "    (32, 38, 'IT-SKILL'),\n",
       "    (40, 50, 'IT-SKILL')]}),\n",
       " ('уверенное знание postman, insomnia;',\n",
       "  {'entities': [(17, 24, 'IT-SKILL'), (26, 34, 'IT-SKILL')]}),\n",
       " ('developed and maintained web applications using php and mysql.',\n",
       "  {'entities': [(35, 38, 'IT-SKILL'), (44, 49, 'IT-SKILL')]}),\n",
       " ('опыт работы с git', {'entities': [(14, 17, 'IT-SKILL')]}),\n",
       " ('мне приходилось работать с различными облачными сервисами, включая aws, azure и google cloud.',\n",
       "  {'entities': [(67, 70, 'IT-SKILL'),\n",
       "    (72, 77, 'IT-SKILL'),\n",
       "    (80, 92, 'IT-SKILL')]}),\n",
       " ('знакомство с различными видами тестирования;',\n",
       "  {'entities': [(31, 43, 'IT-SKILL')]}),\n",
       " ('трекинговые системы jira, clickup, etc;',\n",
       "  {'entities': [(20, 24, 'IT-SKILL'), (26, 33, 'IT-SKILL')]}),\n",
       " ('experience with cloud-native technologies such as kubernetes, docker, and aws lambda.',\n",
       "  {'entities': [(14, 32, 'IT-SKILL'),\n",
       "    (37, 43, 'IT-SKILL'),\n",
       "    (48, 58, 'IT-SKILL'),\n",
       "    (63, 73, 'IT-SKILL')]}),\n",
       " ('веб-разработчик со знанием python, django и react.js',\n",
       "  {'entities': [(0, 15, 'IT-SKILL'),\n",
       "    (27, 33, 'IT-SKILL'),\n",
       "    (35, 41, 'IT-SKILL'),\n",
       "    (44, 52, 'IT-SKILL')]}),\n",
       " ('опыт работы с фреймворками для веб-разработки, такими как django и ruby on rails;',\n",
       "  {'entities': [(13, 28, 'IT-SKILL'),\n",
       "    (49, 55, 'IT-SKILL'),\n",
       "    (61, 75, 'IT-SKILL')]}),\n",
       " ('имею опыт работы с различными базами данных, такими как mysql, postgresql и mongodb.',\n",
       "  {'entities': [(56, 61, 'IT-SKILL'),\n",
       "    (63, 73, 'IT-SKILL'),\n",
       "    (76, 83, 'IT-SKILL')]}),\n",
       " ('я проектировал и разрабатывал системы на python, django и flask.',\n",
       "  {'entities': [(41, 47, 'IT-SKILL'),\n",
       "    (49, 55, 'IT-SKILL'),\n",
       "    (58, 63, 'IT-SKILL')]}),\n",
       " ('знание языка программирования с++;', {'entities': [(7, 31, 'IT-SKILL')]}),\n",
       " ('developed custom wordpress plugins using php and mysql.',\n",
       "  {'entities': [(9, 24, 'IT-SKILL'),\n",
       "    (30, 33, 'IT-SKILL'),\n",
       "    (39, 44, 'IT-SKILL')]}),\n",
       " ('знание javascript-фреймворков react, angular или vue.js',\n",
       "  {'entities': [(7, 34, 'IT-SKILL'),\n",
       "    (36, 43, 'IT-SKILL'),\n",
       "    (48, 54, 'IT-SKILL')]}),\n",
       " ('experience with software testing and test automation using selenium and appium.',\n",
       "  {'entities': [(14, 31, 'IT-SKILL'),\n",
       "    (36, 47, 'IT-SKILL'),\n",
       "    (52, 61, 'IT-SKILL')]}),\n",
       " ('опыт работы с базами данных mongodb, mysql или postgresql',\n",
       "  {'entities': [(25, 32, 'IT-SKILL'),\n",
       "    (34, 38, 'IT-SKILL'),\n",
       "    (43, 53, 'IT-SKILL')]}),\n",
       " ('optimized sql queries and utilized stored procedures',\n",
       "  {'entities': [(10, 13, 'IT-SKILL'), (35, 51, 'IT-SKILL')]}),\n",
       " ('я разрабатывал веб-сайты с использованием html, css и javascript.',\n",
       "  {'entities': [(42, 46, 'IT-SKILL'),\n",
       "    (48, 51, 'IT-SKILL'),\n",
       "    (54, 64, 'IT-SKILL')]}),\n",
       " ('базовое знание sql;', {'entities': [(15, 18, 'IT-SKILL')]}),\n",
       " ('experience with web analytics tools like google analytics and adobe analytics.',\n",
       "  {'entities': [(14, 27, 'IT-SKILL'),\n",
       "    (32, 47, 'IT-SKILL'),\n",
       "    (52, 67, 'IT-SKILL')]}),\n",
       " ('знание принципов solid', {'entities': [(7, 12, 'IT-SKILL')]}),\n",
       " ('реализовал алгоритмы машинного обучения с использованием tensorflow и keras',\n",
       "  {'entities': [(57, 67, 'IT-SKILL'), (70, 75, 'IT-SKILL')]}),\n",
       " ('managed linux servers and wrote shell scripts for automation.',\n",
       "  {'entities': [(8, 13, 'IT-SKILL'),\n",
       "    (14, 21, 'IT-SKILL'),\n",
       "    (32, 37, 'IT-SKILL'),\n",
       "    (38, 45, 'IT-SKILL')]}),\n",
       " ('knowledge of programming languages such as java, python, and c++.',\n",
       "  {'entities': [(22, 26, 'IT-SKILL'),\n",
       "    (28, 34, 'IT-SKILL'),\n",
       "    (39, 42, 'IT-SKILL'),\n",
       "    (47, 49, 'IT-SKILL'),\n",
       "    (54, 57, 'IT-SKILL')]}),\n",
       " ('разработал веб-приложения с использованием angular, node.js и mongodb',\n",
       "  {'entities': [(11, 25, 'IT-SKILL'),\n",
       "    (43, 50, 'IT-SKILL'),\n",
       "    (52, 59, 'IT-SKILL'),\n",
       "    (62, 69, 'IT-SKILL')]}),\n",
       " ('опыт работы с фреймворками symfony или laravel',\n",
       "  {'entities': [(25, 33, 'IT-SKILL'), (38, 45, 'IT-SKILL')]}),\n",
       " ('оптимизировал sql запросы и использовал хранимые процедуры',\n",
       "  {'entities': [(14, 17, 'IT-SKILL'), (40, 58, 'IT-SKILL')]}),\n",
       " ('java software engineer', {'entities': [(0, 4, 'IT-SKILL')]})]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 838
    },
    "id": "OC6gvvNSBGiC",
    "outputId": "9c434d03-cf8c-476a-d484-f2f211a38a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 286.51261815293657}\n",
      "Epoch 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 205.67321458698888}\n",
      "Epoch 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 122.88921398634078}\n",
      "Epoch 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 89.308004245796}\n",
      "Epoch 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 73.07868305866825}\n",
      "Epoch 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 55.16214905816911}\n",
      "Epoch 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 49.54732551724041}\n",
      "Epoch 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 34.621986093101626}\n",
      "Epoch 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 40.38830961629042}\n",
      "Epoch 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 13.582250608972723}\n",
      "Epoch 11: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 17.937318796610572}\n",
      "Epoch 12: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 20.141284444685525}\n",
      "Epoch 13: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 15.19546455748632}\n",
      "Epoch 14: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.21786700801966}\n",
      "Epoch 15: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 13.0170589319413}\n",
      "Epoch 16: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.740500181214281}\n",
      "Epoch 17: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.324341894503667}\n",
      "Epoch 18: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.9715499212550855}\n",
      "Epoch 19: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.2146145637326295}\n",
      "Epoch 20: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.019143515881614}\n"
     ]
    },
    {
     "ename": "ConfigValidationError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigValidationError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-82328f668dd2>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Сохранение обученной модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"it_skills_ner_last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mto_disk\u001b[0;34m(self, path, exclude)\u001b[0m\n\u001b[1;32m   2034\u001b[0m             \u001b[0mserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2035\u001b[0m         \u001b[0mserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2036\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2038\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mto_disk\u001b[0;34m(path, writers, exclude)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m   2026\u001b[0m         )\n\u001b[1;32m   2027\u001b[0m         \u001b[0mserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta.json\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2028\u001b[0;31m         \u001b[0mserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config.cfg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2029\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/confection/__init__.py\u001b[0m in \u001b[0;36mto_disk\u001b[0;34m(self, path, interpolate)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mfile_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     def from_disk(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/confection/__init__.py\u001b[0m in \u001b[0;36mto_str\u001b[0;34m(self, interpolate)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# Order so subsection follow parent (not all sections, then all subs etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mflattened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mstring_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mflattened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/confection/__init__.py\u001b[0m in \u001b[0;36m_validate_sections\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"not part of a section\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"loc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"msg\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdefault_section\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     def from_str(\n",
      "\u001b[0;31mConfigValidationError\u001b[0m: \n\nFound config values without a top-level section\ntraining.batch_size\tnot part of a section\ntraining.dropout\tnot part of a section\ntraining.epochs\tnot part of a section\ntraining.learn_rate\tnot part of a section"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Загрузка предварительно обученной модели SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Подготовка данных для обучения\n",
    "# train_data = [...]  # Ваши данные для обучения\n",
    "\n",
    "\n",
    "# Add a new NER pipeline if it doesn't exist\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "\n",
    "# Add the new label (IT-SKILL) to the NER pipeline\n",
    "ner.add_label(\"IT-SKILL\")\n",
    "\n",
    "# Параметры обучения\n",
    "epochs = 20\n",
    "dropout = 0.2\n",
    "learning_rate = 0.01  # Разные значения learning rate\n",
    "\n",
    "\n",
    "# Инициализация оптимизатора и конфигурация модели\n",
    "optimizer = nlp.resume_training()\n",
    "nlp.config[\"training.batch_size\"] = (1.0, 4.0, 1.001)\n",
    "nlp.config[\"training.dropout\"] = dropout\n",
    "nlp.config[\"training.epochs\"] = epochs\n",
    "nlp.config[\"training.learn_rate\"] = learning_rate\n",
    "\n",
    "# Обучение модели\n",
    "for i in range(epochs):\n",
    "    losses = {}\n",
    "    for batch in minibatch(train_data, size = compounding(4.0, 32.0, 1.001)):\n",
    "        texts, annotations = zip(*batch)\n",
    "        examples = []\n",
    "        for text, annot in zip(texts, annotations):\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annot)\n",
    "            examples.append(example)\n",
    "        nlp.update(examples, drop=dropout, losses=losses, sgd=optimizer)\n",
    "    print(f\"Epoch {i+1}: Losses: {losses}\")\n",
    "\n",
    "\n",
    "# Сохранение обученной модели\n",
    "nlp.to_disk(\"it_skills_ner_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIaatgq4CSMM",
    "outputId": "82714596-2621-4ce8-c68f-28b94c5df5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 289.6494048735}\n",
      "Losses at iteration 1: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 212.34221822424328}\n",
      "Losses at iteration 2: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 134.86793331446472}\n",
      "Losses at iteration 3: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 99.50646493449423}\n",
      "Losses at iteration 4: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 125.22224656598479}\n",
      "Losses at iteration 5: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 97.88313104999212}\n",
      "Losses at iteration 6: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.36701677088963}\n",
      "Losses at iteration 7: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 40.686024747689046}\n",
      "Losses at iteration 8: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 29.35230411417032}\n",
      "Losses at iteration 9: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 28.2531687885893}\n",
      "Losses at iteration 10: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 14.30861875966976}\n",
      "Losses at iteration 11: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 19.840812994862084}\n",
      "Losses at iteration 12: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.316613751199316}\n",
      "Losses at iteration 13: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.37946804422965}\n",
      "Losses at iteration 14: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.920465364646976}\n",
      "Losses at iteration 15: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 8.563071373962382}\n",
      "Losses at iteration 16: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.278690119246445}\n",
      "Losses at iteration 17: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.848124287860872}\n",
      "Losses at iteration 18: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.895011331298046}\n",
      "Losses at iteration 19: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.723396223522395}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a new NER pipeline if it doesn't exist\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "epochs = 20\n",
    "dropout = 0.2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Add the new label (IT-SKILL) to the NER pipeline\n",
    "ner.add_label(\"IT-SKILL\")\n",
    "\n",
    "# Train the NER model\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size = compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in zip(texts, annotations)]\n",
    "        nlp.update(examples, sgd=optimizer, drop=dropout, losses=losses)\n",
    "    print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"it_skills_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srMrDG-JZBjB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUpYL76UJGFd",
    "outputId": "5f9855d3-bab2-49b2-c1eb-f098018eb08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5883.811096946361}\n",
      "Losses at iteration 1: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3390.630333236165}\n",
      "Losses at iteration 2: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2210.6284249590963}\n",
      "Losses at iteration 3: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2821.857988277039}\n",
      "Losses at iteration 4: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1429.9133015221648}\n",
      "Losses at iteration 5: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1448.6831835061664}\n",
      "Losses at iteration 6: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1327.5109940222424}\n",
      "Losses at iteration 7: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 858.5072463889863}\n",
      "Losses at iteration 8: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 990.4950395495677}\n",
      "Losses at iteration 9: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 814.8455280005251}\n",
      "Losses at iteration 10: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 922.3421761783842}\n",
      "Losses at iteration 11: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 688.1679662527847}\n",
      "Losses at iteration 12: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 656.4737102283667}\n",
      "Losses at iteration 13: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 931.137750646373}\n",
      "Losses at iteration 14: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 769.9564940347504}\n",
      "Losses at iteration 15: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 564.6885536143425}\n",
      "Losses at iteration 16: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 607.9297666514608}\n",
      "Losses at iteration 17: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 592.3159275872438}\n",
      "Losses at iteration 18: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 545.579554317559}\n",
      "Losses at iteration 19: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 633.3365961563007}\n",
      "Losses at iteration 20: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 532.0262842906145}\n",
      "Losses at iteration 21: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 647.7224160550269}\n",
      "Losses at iteration 22: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 544.619872345151}\n",
      "Losses at iteration 23: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 780.1884455578507}\n",
      "Losses at iteration 24: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 499.84085894814797}\n",
      "Losses at iteration 25: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 558.7418678962962}\n",
      "Losses at iteration 26: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 470.2882710869609}\n",
      "Losses at iteration 27: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 510.4904648796603}\n",
      "Losses at iteration 28: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 599.4235817293547}\n",
      "Losses at iteration 29: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 454.62985140809457}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "#добавление данных и деление на train и test\n",
    "random.shuffle(TRAIN_DATA)\n",
    "train_size = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:train_size]\n",
    "test_data = TRAIN_DATA[train_size:]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a new NER pipeline if it doesn't exist\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "epochs = 30\n",
    "dropout = 0.35\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Add the new label (IT-SKILL) to the NER pipeline\n",
    "ner.add_label(\"Skills\")\n",
    "\n",
    "# Train the NER model\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size = compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in zip(texts, annotations)]\n",
    "        nlp.update(examples, sgd=optimizer, drop=dropout, losses=losses)\n",
    "    print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"it_skills_ner_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMru7vpAJemR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scKJ_S6Ib1zD",
    "outputId": "5683ae09-9f13-4149-c4ab-feb4a8821250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 194.84451170189425}\n",
      "Losses at iteration 1: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 146.7325428546972}\n",
      "Losses at iteration 2: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 95.68800999119684}\n",
      "Losses at iteration 3: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 79.49926820991651}\n",
      "Losses at iteration 4: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 57.45244025956209}\n",
      "Losses at iteration 5: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 90.14892319212218}\n",
      "Losses at iteration 6: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 67.2847859240016}\n",
      "Losses at iteration 7: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 52.951754887113005}\n",
      "Losses at iteration 8: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 54.212000677888426}\n",
      "Losses at iteration 9: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 46.22611729756545}\n",
      "Losses at iteration 10: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 42.80933296732963}\n",
      "Losses at iteration 11: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 79.29009154548581}\n",
      "Losses at iteration 12: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 44.55341757467626}\n",
      "Losses at iteration 13: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 47.69841507314726}\n",
      "Losses at iteration 14: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 31.410387280400215}\n",
      "Losses at iteration 15: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 39.52699263142771}\n",
      "Losses at iteration 16: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22.585785607873426}\n",
      "Losses at iteration 17: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 26.105139755245766}\n",
      "Losses at iteration 18: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 16.490574554525278}\n",
      "Losses at iteration 19: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 16.625367039945942}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "#добавление данных и деление на train и test\n",
    "random.shuffle(TRAIN_DATA)\n",
    "train_size = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:train_size]\n",
    "test_data = TRAIN_DATA[train_size:]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "epochs = 20\n",
    "dropout = 0.2\n",
    "learning_rate = 0.01\n",
    "\n",
    "ner.add_label(\"IT-SKILL\")\n",
    "\n",
    "optimizer = nlp.resume_training()\n",
    "optimizer.learn_rate = 0.01\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in zip(texts, annotations)]\n",
    "        nlp.update(examples, sgd=optimizer, drop=dropout, losses=losses)\n",
    "    print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "\n",
    "nlp.to_disk(\"it_skills_ner_last_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "kam0YtXseeZe",
    "outputId": "d0a4a04a-362a-41b7-c872-8bc65bfceacb"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-fed6e89f5140>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_vectors_web_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"ner\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_vectors_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "#добавление данных и деление на train и test\n",
    "random.shuffle(TRAIN_DATA)\n",
    "train_size = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:train_size]\n",
    "test_data = TRAIN_DATA[train_size:]\n",
    "\n",
    "nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "epochs = 20\n",
    "dropout = 0.2\n",
    "learning_rate = 0.01\n",
    "\n",
    "ner.add_label(\"IT-SKILL\")\n",
    "\n",
    "optimizer = nlp.resume_training()\n",
    "optimizer.learn_rate = 0.01\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in zip(texts, annotations)]\n",
    "        nlp.update(examples, sgd=optimizer, drop=dropout, losses=losses)\n",
    "    print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "\n",
    "nlp.to_disk(\"it_skills_ner_last_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2XbI27Jd5TU"
   },
   "outputs": [],
   "source": [
    "# print(text)\n",
    "# test_text = \"I have experience in Python, Java, and Ruby on Rails\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "  # print(doc)\n",
    "    if ent.label_ == \"Skills\":\n",
    "        print(f\"IT-SKILL: {ent.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWoIaeEwehrT",
    "outputId": "bfdcc273-7885-4156-b2f9-d298939d870a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    stop_words = set(stopwords.words(\"russian\"))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    gen = \"\"\n",
    "    for i in tokens:\n",
    "      gen = gen + i + \" \"\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpaNa2ymdm5J",
    "outputId": "627df13b-d8be-4a17-d610-568f7c9d5c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6, Recall: 0.16129032258064516, F1: 0.2542372881355932\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluate_ner_model(nlp, test_data)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
